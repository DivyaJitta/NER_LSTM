{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function parses and preprocesses the data and puts them into data structures that can be utilized for the further processing.\n",
    "#Input : path of training_data_file/ validation_data_file/ testing_data_file ( These files can be downloaded  from link provided in README)\n",
    "#output: list of instances, where each instance has 4 lists, representing [words][pos][phrase_chunks][labels]\n",
    "def process_data_file(file_name):\n",
    "    f = open(file_name, \"r\")\n",
    "    tokens = []\n",
    "    pos = []\n",
    "    chunks = []\n",
    "    ner_lables = []\n",
    "    data = []\n",
    "    for line in  f:\n",
    "        if line == \"\\n\" or line.startswith('-DOCSTART'):\n",
    "            if len(tokens) > 0:\n",
    "                sent = []\n",
    "                sent.append(tokens)\n",
    "                sent.append(pos)\n",
    "                sent.append(chunks)\n",
    "                sent.append(ner_lables)\n",
    "                data.append(sent)\n",
    "\n",
    "                # refresh the arrays\n",
    "                tokens = []\n",
    "                pos = []\n",
    "                chunks = []\n",
    "                ner_lables = []\n",
    "\n",
    "        else:\n",
    "            try:\n",
    "                parts = line.strip(\"\\n\").split(\" \")\n",
    "                tokens.append(parts[0])\n",
    "                pos.append(parts[1])\n",
    "                chunks.append(parts[2])\n",
    "                ner_lables.append(parts[3])\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "    #data = data[1:]\n",
    "    data = np.asarray(data)\n",
    "    print(data.shape)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14040, 4)\n",
      "(3249, 4)\n",
      "(3452, 4)\n"
     ]
    }
   ],
   "source": [
    "data = process_data_file(\"NER-datasets/CONLL2003/train.txt\")\n",
    "valid_data = process_data_file(\"NER-datasets/CONLL2003/valid.txt\")\n",
    "test_data = process_data_file(\"NER-datasets/CONLL2003/test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['BRUSSELS', '1996-08-22']), list(['NNP', 'CD']),\n",
       "       list(['B-NP', 'I-NP']), list(['B-LOC', 'O'])], dtype=object)"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function divides the data into batches equal size\n",
    "# Input: training data (array of instances)\n",
    "# Output: list of batches that are of equal size\n",
    "def createEqualBatches(data):\n",
    "    n_batches = 100\n",
    "    batch_size = len(data) // n_batches\n",
    "    #print(batch_size)\n",
    "    indices = []\n",
    "    for i in range(n_batches):\n",
    "        indices.append(batch_size*(i+1))\n",
    "    #print(indices)\n",
    "    \n",
    "    batches = []\n",
    "    batch_len = []\n",
    "    z = 0\n",
    "    start = 0\n",
    "    print(len(indices))\n",
    "    for end in indices:\n",
    "        #print(\"start, end\", start, end)\n",
    "        batches.append(data[start:end])\n",
    "        start = end\n",
    "\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "batches = createEqualBatches(data)\n",
    "print(len(batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[list(['Peter', 'Blackburn']) list(['NNP', 'NNP']) list(['B-NP', 'I-NP'])\n",
      " list(['B-PER', 'I-PER'])]\n"
     ]
    }
   ],
   "source": [
    "print(data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intializing some global vars\n",
    "# Reads from the glove embedding file and loads the embeddings into a dictionary and stores them in a pickle file\n",
    "#Input: glove embeddings file\n",
    "#Output: pickled embeddings\n",
    "# TO DO : MAKE A DIRECTORY \"embeddings\"\n",
    "Embeddings = {}\n",
    "word_emb_dim  = 50\n",
    "embedding_file = \"embeddings/glove.6B.50d.txt\"\n",
    "def LoadEmbeddings(embedding_file):\n",
    "    global Embeddings, word_emb_dim\n",
    "    f = open(embedding_file, \"r\", encoding= \"utf-8\", errors= \"ignore\")\n",
    "    for line in f :\n",
    "        tokens = line.strip(\"\\n\").split()\n",
    "        word = tokens[0].lower()\n",
    "        vec = tokens[1:]\n",
    "        vec = \" \". join(vec)\n",
    "        Embeddings[word] = vec\n",
    "    Embeddings[\"zero_vec\"] = \"0.0 \" * word_emb_dim\n",
    "    Embeddings[\"zero_vec\"] = Embeddings[\"zero_vec\"].rstrip()\n",
    "    f.close()\n",
    "    \n",
    "    g = open (\"embeddings/EmbedDict.pkl\", \"wb\")\n",
    "    pickle.dump(Embeddings, g)\n",
    "    g.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "LoadEmbeddings(\"embeddings/glove.6B.50d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "GloveEmbeddings = {}\n",
    "pos_emebddings = {}\n",
    "max_words = 15\n",
    "case_emb_dim = 8\n",
    "pos_emb_dim = 40\n",
    "num_classes =  10\n",
    "\n",
    "# Loads the word embeddings into a dictionary\n",
    "def load_word_embeddings():\n",
    "    #load the embeddings from the pickle file\n",
    "    global GloveEmbeddings\n",
    "    f = open(\"embeddings/EmbedDict.pkl\", \"rb\")\n",
    "    GloveEmbeddings = pickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "# gets the word features\n",
    "# Input: List of words and left_over is an integer that says how much padding is required\n",
    "# Output: list of feature vectors for the words/ sentence ( with pads)\n",
    "def get_word_feat_vecs(words, left_over):\n",
    "    global max_words, GloveEmbeddings\n",
    "    #remove extra words\n",
    "    #print(words, left_over)\n",
    "    if left_over < 0:\n",
    "        words = words[:max_words]\n",
    "    elif(left_over > 0): #padding req\n",
    "        for i in range(left_over):\n",
    "            words.append(\"zero_vec\")\n",
    "    \n",
    "    # now obtain the feature vector\n",
    "    feat_vec = []\n",
    "    for word in words:\n",
    "        word = word.lower()\n",
    "        try:\n",
    "            glove_vec = []\n",
    "            splits = GloveEmbeddings[word].split()\n",
    "            #print(splits)\n",
    "            for v in splits:\n",
    "                glove_vec.append(float(v))\n",
    "            #glove_vec = [float(v for v in GloveEmbeddings[word].split())]\n",
    "        except:\n",
    "            glove_vec = []\n",
    "            splits = GloveEmbeddings[\"zero_vec\"].split()\n",
    "            #print(splits)\n",
    "            for v in splits:\n",
    "                glove_vec.append(float(v))\n",
    "        feat_vec.append(glove_vec)\n",
    "    return feat_vec\n",
    "\n",
    "#Gets the case embedding (Also one of the features)\n",
    "#Input : A single word\n",
    "#Output: Case embedding for that\n",
    "def  get_case_embeddings(word):\n",
    "    global case_emb_dim\n",
    "    case_map = {'numeric': 0, 'all_lower': 1, 'all_upper': 2, 'initial_upper': 3, 'other': 4, 'mainly_numeric': 5,\n",
    "                    'contains_digit': 6, 'PADDED_TOKEN': 7}\n",
    "    num_of_digits = 0\n",
    "    for char in word:\n",
    "        if char.isdigit():\n",
    "            num_of_digits += 1\n",
    "    number_part = float(num_of_digits/len(word))\n",
    "    \n",
    "    casing = 'other'\n",
    "    \n",
    "    if num_of_digits == len(word):\n",
    "        casing = \"numeric\"\n",
    "    elif number_part >= 0.5:\n",
    "        casing = \"mainly_numeric\"\n",
    "    elif num_of_digits > 0:\n",
    "        casing = \"contains_digit\"\n",
    "    elif word.isupper():\n",
    "        casing = \"all_upper\"\n",
    "    elif word.islower():\n",
    "        casing = \"all_lower\"\n",
    "    elif word[0].lower():\n",
    "        casing = \"initial_upper\"\n",
    "    elif word[0].lower() == \"zero_vec\":\n",
    "        casing = \"PADDING_TOKEN\"\n",
    "    \n",
    "    case_vector = [0]*case_emb_dim\n",
    "    case_vector[case_map[casing]] = 1\n",
    "    \n",
    "    return case_vector\n",
    "\n",
    "###################### Create pickle files for  pos and label embeddings #####################\n",
    "\n",
    "def create_pos_embeddings(data):\n",
    "    print(len(data))\n",
    "    pos_statistics = {}\n",
    "    pos_set = []\n",
    "    for instance in data:\n",
    "        pos_tags = instance[1]\n",
    "        #print(pos_tags)\n",
    "        for tag in pos_tags:\n",
    "            if tag in string.punctuation:\n",
    "                tag = \"PUNCT\"\n",
    "            try:\n",
    "                pos_statistics[tag] = pos_statistics[tag] + 1\n",
    "            except:\n",
    "                pos_statistics[tag] = 1\n",
    "    \n",
    "    print(pos_statistics)\n",
    "    for pos in pos_statistics:\n",
    "        pos_set.append(pos)\n",
    "    pos_set.append(\"PADDED_POS_TAG\")\n",
    "    \n",
    "    print(pos_set)\n",
    "    print(len(pos_set))\n",
    "    pos_map = {}\n",
    "    for pos_idx in range(len(pos_set)):\n",
    "        pos_vec = [0]* len(pos_set)\n",
    "        pos_vec[pos_idx] = 1\n",
    "        pos_map[pos_set[pos_idx]] = pos_vec\n",
    "    \n",
    "    f = open(\"POSEmbeddings.pickle\", \"wb\")\n",
    "    pickle.dump(pos_map, f)\n",
    "    f.close()\n",
    "    \n",
    "def create_label_embeddings(data):\n",
    "    label_set = []\n",
    "    label_statistics = {}\n",
    "    for instance in data:\n",
    "        labels = instance[3]\n",
    "        for label in labels:\n",
    "            try:\n",
    "                label_statistics[label] = label_statistics[label] + 1\n",
    "            except:\n",
    "                label_statistics[label] = 1\n",
    "    ############### COMMENT WHEN LABEL STATISTICS IS NOT NEEDED ##################\n",
    "    print(label_statistics)\n",
    "    total_count = 0\n",
    "    for label in label_statistics:\n",
    "        total_count += label_statistics[label]\n",
    "    \n",
    "    for label, count in label_statistics.items():\n",
    "        print(\"{}: {}%\".format(label, round((count/total_count)*100, 2)))\n",
    "    #####################################################################################\n",
    "#     for label in label_statistics:\n",
    "#         label_set.append(label)\n",
    "#     label_set.append(\"PADDED_LABEL\")\n",
    "#     print(label_set)\n",
    "#     print(len(label_set))\n",
    "    \n",
    "#     label_map = {}\n",
    "    \n",
    "#     for label_idx in range(len(label_set)):\n",
    "#         label_vec = [0] * len(label_set)\n",
    "#         label_vec[label_idx] = 1\n",
    "#         label_map[label_set[label_idx]] = label_vec\n",
    "    \n",
    "#     f = open(\"LabelEmbed.pickle\", \"wb\")\n",
    "#     pickle.dump(label_map, f)\n",
    "#     f.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B-ORG': 6319, 'O': 169576, 'B-MISC': 3438, 'B-PER': 6600, 'I-PER': 4528, 'B-LOC': 7140, 'I-ORG': 3704, 'I-MISC': 1155, 'I-LOC': 1157}\n",
      "B-ORG: 3.1%\n",
      "O: 83.28%\n",
      "B-MISC: 1.69%\n",
      "B-PER: 3.24%\n",
      "I-PER: 2.22%\n",
      "B-LOC: 3.51%\n",
      "I-ORG: 1.82%\n",
      "I-MISC: 0.57%\n",
      "I-LOC: 0.57%\n",
      "\n",
      "\n",
      "{'O': 42757, 'B-ORG': 1340, 'B-LOC': 1837, 'B-MISC': 922, 'I-MISC': 346, 'B-PER': 1842, 'I-PER': 1307, 'I-LOC': 257, 'I-ORG': 750}\n",
      "O: 83.25%\n",
      "B-ORG: 2.61%\n",
      "B-LOC: 3.58%\n",
      "B-MISC: 1.8%\n",
      "I-MISC: 0.67%\n",
      "B-PER: 3.59%\n",
      "I-PER: 2.54%\n",
      "I-LOC: 0.5%\n",
      "I-ORG: 1.46%\n",
      "\n",
      "\n",
      "{'O': 38289, 'B-LOC': 1667, 'B-PER': 1616, 'I-PER': 1156, 'I-LOC': 257, 'B-MISC': 701, 'I-MISC': 214, 'B-ORG': 1660, 'I-ORG': 834}\n",
      "O: 82.53%\n",
      "B-LOC: 3.59%\n",
      "B-PER: 3.48%\n",
      "I-PER: 2.49%\n",
      "I-LOC: 0.55%\n",
      "B-MISC: 1.51%\n",
      "I-MISC: 0.46%\n",
      "B-ORG: 3.58%\n",
      "I-ORG: 1.8%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Gives the class statistics for Train_Data, Valid_Data, Test_Data\n",
    "create_label_embeddings(data)\n",
    "print(\"\\n\")\n",
    "create_label_embeddings(valid_data)\n",
    "print(\"\\n\")\n",
    "create_label_embeddings(test_data)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-ORG', 'O', 'B-MISC', 'B-PER', 'I-PER', 'B-LOC', 'I-ORG', 'I-MISC', 'I-LOC', 'PADDED_LABEL']\n",
      "10\n",
      "14040\n",
      "{'NNP': 34391, 'VBZ': 2426, 'JJ': 11831, 'NN': 23898, 'TO': 3469, 'VB': 4252, 'PUNCT': 25403, 'CD': 19702, 'DT': 13453, 'VBD': 8293, 'IN': 19064, 'PRP': 3163, 'NNS': 9903, 'VBP': 1436, 'MD': 1199, 'VBN': 4105, 'POS': 1553, 'JJR': 382, 'RB': 3975, 'FW': 166, 'CC': 3653, 'WDT': 506, 'PRP$': 1520, 'RBR': 163, 'VBG': 2585, 'EX': 136, 'WP': 528, 'WRB': 384, 'RP': 528, 'NNPS': 684, 'SYM': 439, 'RBS': 35, 'UH': 30, 'PDT': 33, \"''\": 35, 'LS': 13, 'JJS': 254, 'WP$': 23, 'NN|SYM': 4}\n",
      "['NNP', 'VBZ', 'JJ', 'NN', 'TO', 'VB', 'PUNCT', 'CD', 'DT', 'VBD', 'IN', 'PRP', 'NNS', 'VBP', 'MD', 'VBN', 'POS', 'JJR', 'RB', 'FW', 'CC', 'WDT', 'PRP$', 'RBR', 'VBG', 'EX', 'WP', 'WRB', 'RP', 'NNPS', 'SYM', 'RBS', 'UH', 'PDT', \"''\", 'LS', 'JJS', 'WP$', 'NN|SYM', 'PADDED_POS_TAG']\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "create_label_embeddings(data) # Label Statistics\n",
    "create_pos_embeddings(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.502635327635328\n",
      "{9: 894, 2: 1129, 30: 194, 31: 209, 33: 173, 25: 223, 40: 80, 28: 217, 37: 102, 27: 202, 1: 179, 26: 212, 35: 135, 39: 109, 34: 155, 15: 279, 16: 235, 10: 503, 32: 186, 12: 324, 8: 1056, 47: 21, 24: 221, 44: 45, 42: 64, 11: 407, 19: 195, 21: 213, 14: 293, 17: 233, 29: 216, 22: 217, 3: 586, 7: 904, 13: 354, 5: 751, 20: 214, 6: 625, 46: 27, 38: 98, 36: 124, 18: 198, 4: 770, 52: 11, 23: 210, 41: 72, 50: 15, 45: 40, 54: 4, 43: 52, 48: 18, 51: 10, 58: 2, 57: 3, 60: 2, 49: 13, 53: 3, 55: 5, 62: 2, 78: 1, 56: 1, 59: 2, 113: 1, 67: 1}\n"
     ]
    }
   ],
   "source": [
    "#gives average length of the sentence in trainig  data, useful to fix on a sentence length\n",
    "def average_sent_len(data):\n",
    "    word_lens = []\n",
    "    len_dict ={}\n",
    "    for instance in data:\n",
    "        words = instance[0]\n",
    "        word_lens.append(len(words))\n",
    "    print(float(sum(word_lens)/len(word_lens)))\n",
    "    for length in word_lens:\n",
    "        try:\n",
    "            len_dict[length] += 1\n",
    "        except:\n",
    "            len_dict[length] = 1\n",
    "    print(len_dict)\n",
    "\n",
    "average_sent_len(data) #Average length of the snetences\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gives how many words are needded to be padded given max words a sentence can take is  15.\n",
    "# Input: List of words [Represents a sentence]\n",
    "# output: Padding required (Integer Value)\n",
    "def remaining(words):\n",
    "    global max_words\n",
    "    word_count = len(words)\n",
    "    left_over = max_words - word_count\n",
    "    return left_over\n",
    "# Gets label vectors\n",
    "def get_label_vectors(data):\n",
    "    global num_classes, max_words\n",
    "    data_label_vectors = []\n",
    "    f = open(\"LabelEmbed.pickle\", \"rb\")\n",
    "    label_map = pickle.load(f)\n",
    "    f.close()\n",
    "    \n",
    "    for  instance in data:\n",
    "        labels = instance[3]\n",
    "        left_over = remaining(labels)\n",
    "        if left_over < 0:\n",
    "            #print(\"no pad\")\n",
    "            labels =  labels[:max_words]\n",
    "        temp = []\n",
    "        for label in labels:\n",
    "            label_vec = label_map[label]\n",
    "            temp.append(label_vec)\n",
    "        if left_over > 0:\n",
    "            #print(\"Padded\")\n",
    "            label_vec = [0] * num_classes\n",
    "            label_vec[9] = 1\n",
    "            for i in range(left_over):\n",
    "                temp.append(label_vec)\n",
    "        #print(len(temp))\n",
    "        #print(np.asarray(temp).shape)\n",
    "        data_label_vectors.append(temp)\n",
    "    \n",
    "    return data_label_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14040, 15, 10)\n"
     ]
    }
   ],
   "source": [
    "data_label_vecs = get_label_vectors(data)\n",
    "a = np.asarray(data_label_vecs)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gives case features\n",
    "#Input: list of words, padding_req\n",
    "#Output: list of case vectors\n",
    "\n",
    "def get_case_feat_vecs(words, left_over):\n",
    "    global case_emb_dim, max_words\n",
    "    case_vecs = []\n",
    "    if left_over < 0:\n",
    "        words = words[:max_words]\n",
    "    for word in words:\n",
    "        case_embedding = get_case_embeddings(word)\n",
    "        case_vecs.append(case_embedding)\n",
    "    if left_over > 0:\n",
    "        case_embedding = [0] * case_emb_dim\n",
    "        case_embedding[7] = 1\n",
    "        for i in range(left_over):\n",
    "            case_vecs.append(case_embedding)\n",
    "    return case_vecs\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_case_feat_vecs() missing 1 required positional argument: 'left_over'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-85-8ca169b2e8af>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_case_feat_vecs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: get_case_feat_vecs() missing 1 required positional argument: 'left_over'"
     ]
    }
   ],
   "source": [
    "print(len(get_case_feat_vecs(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pos_embeddings():\n",
    "    global pos_embeddings\n",
    "    f = open(\"POSEmbeddings.pickle\", \"rb\")\n",
    "    pos_embeddings = pickle.load(f)\n",
    "    f.close()\n",
    "    \n",
    "def get_pos_feat_vecs(pos, left_over):\n",
    "    global pos_embeddings, pos_emb_dim, max_words\n",
    "    pos_vecs = []\n",
    "    if left_over < 0:\n",
    "        pos = pos[:max_words]\n",
    "    for tag in pos:\n",
    "        if tag in string.punctuation:\n",
    "            tag = \"PUNCT\"\n",
    "        pos_vec = pos_embeddings[tag]\n",
    "        pos_vecs.append(pos_vec)\n",
    "    if left_over > 0:\n",
    "        pos_vec = pos_embeddings[\"PADDED_POS_TAG\"]\n",
    "        for i in range(left_over):\n",
    "            pos_vecs.append(pos_vec)\n",
    "    return pos_vecs\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_pos_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_vectors(data):\n",
    "    ## This function gets the feature vectors (word, case and pos) for the entire training data ###\n",
    "    \n",
    "    # get feats for words\n",
    "    global max_words\n",
    "    data_word_feats = []\n",
    "    data_case_feats = []\n",
    "    data_pos_feats = []\n",
    "    load_word_embeddings()\n",
    "    load_pos_embeddings()\n",
    "    for instance in data:\n",
    "        tokens = instance[0]\n",
    "        pos = instance[1]\n",
    "#         if (len(tokens) != len(pos)):\n",
    "#             print(\"WRONG\")\n",
    "#             print(tokens)\n",
    "#             print(pos)\n",
    "        left_over = remaining(tokens)\n",
    "        \n",
    "       \n",
    "        #get feats for POS\n",
    "        pos_feat_vecs = get_pos_feat_vecs(pos, left_over)\n",
    "        data_pos_feats.append(pos_feat_vecs)\n",
    "        #get feats for case\n",
    "        case_feat_vecs = get_case_feat_vecs(tokens, left_over)\n",
    "        data_case_feats.append(case_feat_vecs)\n",
    "        #get for words\n",
    "        words_feat_vecs = get_word_feat_vecs(tokens, left_over)\n",
    "        data_word_feats.append(words_feat_vecs)\n",
    "        \n",
    "       \n",
    "    \n",
    "    return data_word_feats, data_case_feats, data_pos_feats\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model, load_model, save_model\n",
    "from keras.layers import Bidirectional, LSTM, Dense, Activation, Input, concatenate, TimeDistributed\n",
    "def Build_Model(sent_max_words = 15, word_emb_dim = 50 , case_emb_dim = 7, pos_emb_dim = 46):\n",
    "    lstm_dim = 200\n",
    "    global num_classes\n",
    "    #word input\n",
    "    word_input = Input(shape=(sent_max_words, word_emb_dim))\n",
    "    #case input\n",
    "    case_input = Input(shape=(sent_max_words, case_emb_dim))\n",
    "    #pos_input\n",
    "    pos_input = Input(shape=(sent_max_words, pos_emb_dim))\n",
    "    #Concatenate the three inputs\n",
    "    merged_input = concatenate([word_input, case_input, pos_input])\n",
    "    #pass the merged input to a BiLSTM\n",
    "    lstm_output = Bidirectional(LSTM(lstm_dim, return_sequences=True, dropout = 0.2),merge_mode=None)(merged_input)\n",
    "    merged_output = concatenate([lstm_output[0], lstm_output[1]], axis = 2)\n",
    "    #Add a Time Distributed Layer\n",
    "    output = TimeDistributed(Dense(num_classes, activation = \"softmax\"), name = \"Softmax_Layer\")(merged_output)\n",
    "    \n",
    "    ## Model ##\n",
    "    model = Model(inputs = [word_input, case_input, pos_input], outputs = output)\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 15, 50)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 15, 7)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 15, 46)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 15, 103)      0           input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) [(None, 15, 200), (N 486400      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 15, 400)      0           bidirectional_1[0][0]            \n",
      "                                                                 bidirectional_1[0][1]            \n",
      "__________________________________________________________________________________________________\n",
      "Softmax_Layer (TimeDistributed) (None, 15, 10)       4010        concatenate_2[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 490,410\n",
      "Trainable params: 490,410\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.training.Model at 0x2c6ee5e6358>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Build_Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14040, 4)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_22 (InputLayer)           (None, 15, 50)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_23 (InputLayer)           (None, 15, 7)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_24 (InputLayer)           (None, 15, 39)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)    (None, 15, 96)       0           input_22[0][0]                   \n",
      "                                                                 input_23[0][0]                   \n",
      "                                                                 input_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_8 (Bidirectional) [(None, 15, 200), (N 475200      concatenate_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_16 (Concatenate)    (None, 15, 400)      0           bidirectional_8[0][0]            \n",
      "                                                                 bidirectional_8[0][1]            \n",
      "__________________________________________________________________________________________________\n",
      "Softmax_Layer (TimeDistributed) (None, 15, 10)       4010        concatenate_16[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 479,210\n",
      "Trainable params: 479,210\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "10\n",
      "14040\n",
      "(14040, 15, 50)\n",
      "(14040, 15, 7)\n",
      "(14040, 15, 39)\n",
      "(14040, 15)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected Softmax_Layer to have 3 dimensions, but got array with shape (14040, 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-108-8d385715133d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-108-8d385715133d>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos_feats\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_label_vectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword_feats\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcase_feats\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos_feats\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_label_vectors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m149\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtotal_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;31m#     for epoch in  range(total_epochs):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;31m#         model.fit([word_feats, case_feats, pos_feats], train_label_vectors, batch_size = 149, validation_split = 0)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\z003ws3e\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 952\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m    953\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\z003ws3e\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 789\u001b[1;33m                 exception_prefix='target')\n\u001b[0m\u001b[0;32m    790\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m             \u001b[1;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\z003ws3e\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    126\u001b[0m                         \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' dimensions, but got array '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[0;32m    129\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking target: expected Softmax_Layer to have 3 dimensions, but got array with shape (14040, 15)"
     ]
    }
   ],
   "source": [
    "# Do not RUN #\n",
    "#get epoch wise train error and validation error\n",
    "import numpy as np\n",
    "def train_model(data):\n",
    "    print(data.shape)\n",
    "    global max_words, word_emb_dim, case_emb_dim, pos_emb_dim, num_classes\n",
    "    total_epochs = 30\n",
    "    batch_size = 149 #equal batches\n",
    "    #batches = createEqualBatches(data)\n",
    "    model = Build_Model(max_words, word_emb_dim, case_emb_dim, pos_emb_dim)\n",
    "    model.compile(loss= \"categorical_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n",
    "    train_label_vectors = get_label_vectors(data)\n",
    "    word_feats, case_feats, pos_feats =  get_feature_vectors(data)\n",
    "    #print(len(word_feats), len(case_feats), len(pos_feats))\n",
    "    \n",
    "    print(len(train_label_vectors))\n",
    "    \n",
    "    #Metrics for each epoch\n",
    "    word_feats = np.asarray(word_feats)\n",
    "    case_feats = np.asarray(case_feats)\n",
    "    pos_feats = np.asarray(pos_feats)\n",
    "    train_label_vectors = np.asarray(train_label_vectors)\n",
    "    print(word_feats.shape)\n",
    "    print(case_feats.shape)\n",
    "    print(pos_feats.shape)\n",
    "    print(train_label_vectors.shape)\n",
    "    model.fit([word_feats, case_feats, pos_feats], train_label_vectors, batch_size = 149, validation_split = 0.2, initial_epoch=0, epochs = total_epochs)\n",
    "#     for epoch in  range(total_epochs):\n",
    "#         model.fit([word_feats, case_feats, pos_feats], train_label_vectors, batch_size = 149, validation_split = 0)\n",
    "\n",
    "        \n",
    "\n",
    "train_model(data)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14040, 4)\n",
      "(3249, 4)\n"
     ]
    }
   ],
   "source": [
    "#Parsing and preprocessing of the validation_data\n",
    "#ALWAYS RUN BEFORE GETTING THE RESULTS AND TRAINING THE MODEL\n",
    "data = process_data_file(\"NER-datasets/CONLL2003/train.txt\")\n",
    "valid_data = process_data_file(\"NER-datasets/CONLL2003/valid.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FOR THE PURPOSE OF USING SKLEARN\n",
    "def convert_to_scalar(label_vectors):\n",
    "    scalar_vec = []\n",
    "    for sentence in label_vectors:\n",
    "        labels = []\n",
    "        for word in sentence:\n",
    "            #print(word)\n",
    "            idx = np.argmax(word)\n",
    "            labels.append(idx)\n",
    "        scalar_vec.append(labels)\n",
    "    scalar_vec = np.asarray(scalar_vec)\n",
    "    \n",
    "    return scalar_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14040, 4)\n",
      "(3249, 4)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 15, 50)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            (None, 15, 8)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_9 (InputLayer)            (None, 15, 40)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 15, 98)       0           input_7[0][0]                    \n",
      "                                                                 input_8[0][0]                    \n",
      "                                                                 input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) [(None, 15, 200), (N 478400      concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 15, 400)      0           bidirectional_3[0][0]            \n",
      "                                                                 bidirectional_3[0][1]            \n",
      "__________________________________________________________________________________________________\n",
      "Softmax_Layer (TimeDistributed) (None, 15, 10)       4010        concatenate_6[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 482,410\n",
      "Trainable params: 482,410\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "(14040, 15, 50) (14040, 15, 8) (14040, 15, 40) (14040, 15, 10)\n",
      "(3249, 15, 50) (3249, 15, 8) (3249, 15, 40) (3249, 15, 10)\n",
      "Train on 14040 samples, validate on 3249 samples\n",
      "Epoch 1/50\n",
      "14040/14040 [==============================] - 28s 2ms/step - loss: 0.5436 - acc: 0.8645 - val_loss: 0.2354 - val_acc: 0.9360\n",
      "Epoch 2/50\n",
      "14040/14040 [==============================] - 21s 2ms/step - loss: 0.1719 - acc: 0.9513 - val_loss: 0.1274 - val_acc: 0.9649\n",
      "Epoch 3/50\n",
      "14040/14040 [==============================] - 21s 1ms/step - loss: 0.1144 - acc: 0.9673 - val_loss: 0.1014 - val_acc: 0.9714\n",
      "Epoch 4/50\n",
      "14040/14040 [==============================] - 22s 2ms/step - loss: 0.0938 - acc: 0.9731 - val_loss: 0.0869 - val_acc: 0.9754\n",
      "Epoch 5/50\n",
      "14040/14040 [==============================] - 21s 1ms/step - loss: 0.0811 - acc: 0.9768 - val_loss: 0.0802 - val_acc: 0.9764\n",
      "Epoch 6/50\n",
      "14040/14040 [==============================] - 21s 1ms/step - loss: 0.0714 - acc: 0.9794 - val_loss: 0.0681 - val_acc: 0.9811\n",
      "Epoch 7/50\n",
      "14040/14040 [==============================] - 21s 2ms/step - loss: 0.0635 - acc: 0.9816 - val_loss: 0.0636 - val_acc: 0.9819\n",
      "Epoch 8/50\n",
      "14040/14040 [==============================] - 21s 1ms/step - loss: 0.0588 - acc: 0.9827 - val_loss: 0.0598 - val_acc: 0.9829\n",
      "Epoch 9/50\n",
      "14040/14040 [==============================] - 21s 2ms/step - loss: 0.0532 - acc: 0.9846 - val_loss: 0.0572 - val_acc: 0.9830\n",
      "Epoch 10/50\n",
      "14040/14040 [==============================] - 22s 2ms/step - loss: 0.0489 - acc: 0.9856 - val_loss: 0.0588 - val_acc: 0.9830\n",
      "Epoch 11/50\n",
      "14040/14040 [==============================] - 21s 1ms/step - loss: 0.0448 - acc: 0.9869 - val_loss: 0.0526 - val_acc: 0.9845\n",
      "Epoch 12/50\n",
      "14040/14040 [==============================] - 21s 2ms/step - loss: 0.0407 - acc: 0.9882 - val_loss: 0.0503 - val_acc: 0.9856\n",
      "Epoch 13/50\n",
      "14040/14040 [==============================] - 22s 2ms/step - loss: 0.0369 - acc: 0.9892 - val_loss: 0.0501 - val_acc: 0.9857\n",
      "Epoch 14/50\n",
      "14040/14040 [==============================] - 21s 2ms/step - loss: 0.0347 - acc: 0.9897 - val_loss: 0.0491 - val_acc: 0.9860\n",
      "Epoch 15/50\n",
      "14040/14040 [==============================] - 21s 2ms/step - loss: 0.0318 - acc: 0.9907 - val_loss: 0.0485 - val_acc: 0.9864\n",
      "Epoch 16/50\n",
      "14040/14040 [==============================] - 22s 2ms/step - loss: 0.0295 - acc: 0.9912 - val_loss: 0.0485 - val_acc: 0.9863\n",
      "Epoch 17/50\n",
      "14040/14040 [==============================] - 21s 1ms/step - loss: 0.0275 - acc: 0.9919 - val_loss: 0.0493 - val_acc: 0.9857\n",
      "Epoch 18/50\n",
      "14040/14040 [==============================] - 22s 2ms/step - loss: 0.0254 - acc: 0.9926 - val_loss: 0.0461 - val_acc: 0.9874\n",
      "Epoch 19/50\n",
      "14040/14040 [==============================] - 21s 2ms/step - loss: 0.0230 - acc: 0.9933 - val_loss: 0.0473 - val_acc: 0.9867\n",
      "Epoch 20/50\n",
      "14040/14040 [==============================] - 21s 2ms/step - loss: 0.0215 - acc: 0.9938 - val_loss: 0.0491 - val_acc: 0.9860\n",
      "Epoch 21/50\n",
      "14040/14040 [==============================] - 21s 2ms/step - loss: 0.0198 - acc: 0.9943 - val_loss: 0.0479 - val_acc: 0.9868\n",
      "Epoch 22/50\n",
      "14040/14040 [==============================] - 21s 1ms/step - loss: 0.0182 - acc: 0.9948 - val_loss: 0.0487 - val_acc: 0.9868\n",
      "Epoch 23/50\n",
      "14040/14040 [==============================] - 22s 2ms/step - loss: 0.0173 - acc: 0.9950 - val_loss: 0.0481 - val_acc: 0.9868\n",
      "Epoch 24/50\n",
      "14040/14040 [==============================] - 23s 2ms/step - loss: 0.0153 - acc: 0.9957 - val_loss: 0.0476 - val_acc: 0.9871\n",
      "Epoch 25/50\n",
      "14040/14040 [==============================] - 21s 2ms/step - loss: 0.0143 - acc: 0.9959 - val_loss: 0.0475 - val_acc: 0.9870\n",
      "Epoch 26/50\n",
      "14040/14040 [==============================] - 22s 2ms/step - loss: 0.0133 - acc: 0.9964 - val_loss: 0.0491 - val_acc: 0.9869\n",
      "Epoch 27/50\n",
      "14040/14040 [==============================] - 22s 2ms/step - loss: 0.0123 - acc: 0.9966 - val_loss: 0.0476 - val_acc: 0.9873\n",
      "Epoch 28/50\n",
      "14040/14040 [==============================] - 22s 2ms/step - loss: 0.0108 - acc: 0.9971 - val_loss: 0.0487 - val_acc: 0.9874\n",
      "Epoch 29/50\n",
      "14040/14040 [==============================] - 22s 2ms/step - loss: 0.0104 - acc: 0.9973 - val_loss: 0.0498 - val_acc: 0.9870\n",
      "Epoch 30/50\n",
      "14040/14040 [==============================] - 22s 2ms/step - loss: 0.0101 - acc: 0.9973 - val_loss: 0.0507 - val_acc: 0.9871\n",
      "Epoch 31/50\n",
      "14040/14040 [==============================] - 22s 2ms/step - loss: 0.0095 - acc: 0.9974 - val_loss: 0.0489 - val_acc: 0.9878\n",
      "Epoch 32/50\n",
      "14040/14040 [==============================] - 22s 2ms/step - loss: 0.0085 - acc: 0.9977 - val_loss: 0.0502 - val_acc: 0.9879\n",
      "Epoch 33/50\n",
      "14040/14040 [==============================] - 22s 2ms/step - loss: 0.0079 - acc: 0.9980 - val_loss: 0.0523 - val_acc: 0.9878\n",
      "Epoch 34/50\n",
      "14040/14040 [==============================] - 22s 2ms/step - loss: 0.0074 - acc: 0.9982 - val_loss: 0.0528 - val_acc: 0.9872\n",
      "Epoch 35/50\n",
      "14040/14040 [==============================] - 22s 2ms/step - loss: 0.0066 - acc: 0.9984 - val_loss: 0.0509 - val_acc: 0.9878\n",
      "Epoch 36/50\n",
      "14040/14040 [==============================] - 22s 2ms/step - loss: 0.0063 - acc: 0.9985 - val_loss: 0.0529 - val_acc: 0.9874\n",
      "Epoch 37/50\n",
      "14040/14040 [==============================] - 22s 2ms/step - loss: 0.0059 - acc: 0.9985 - val_loss: 0.0521 - val_acc: 0.9875\n",
      "Epoch 38/50\n",
      "14040/14040 [==============================] - 22s 2ms/step - loss: 0.0054 - acc: 0.9987 - val_loss: 0.0536 - val_acc: 0.9874\n",
      "Epoch 39/50\n",
      "14040/14040 [==============================] - 22s 2ms/step - loss: 0.0053 - acc: 0.9987 - val_loss: 0.0528 - val_acc: 0.9873\n",
      "Epoch 40/50\n",
      "14040/14040 [==============================] - 21s 2ms/step - loss: 0.0049 - acc: 0.9988 - val_loss: 0.0520 - val_acc: 0.9876\n",
      "Epoch 41/50\n",
      "14040/14040 [==============================] - 21s 2ms/step - loss: 0.0048 - acc: 0.9988 - val_loss: 0.0571 - val_acc: 0.9874\n",
      "Epoch 42/50\n",
      "14040/14040 [==============================] - 21s 2ms/step - loss: 0.0043 - acc: 0.9990 - val_loss: 0.0550 - val_acc: 0.9878\n",
      "Epoch 43/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14040/14040 [==============================] - 20s 1ms/step - loss: 0.0042 - acc: 0.9990 - val_loss: 0.0573 - val_acc: 0.9876\n",
      "Epoch 44/50\n",
      "14040/14040 [==============================] - 21s 2ms/step - loss: 0.0038 - acc: 0.9991 - val_loss: 0.0568 - val_acc: 0.9878\n",
      "Epoch 45/50\n",
      "14040/14040 [==============================] - 21s 1ms/step - loss: 0.0037 - acc: 0.9992 - val_loss: 0.0592 - val_acc: 0.9871\n",
      "Epoch 46/50\n",
      "14040/14040 [==============================] - 21s 1ms/step - loss: 0.0036 - acc: 0.9992 - val_loss: 0.0586 - val_acc: 0.9878\n",
      "Epoch 47/50\n",
      "14040/14040 [==============================] - 21s 1ms/step - loss: 0.0037 - acc: 0.9991 - val_loss: 0.0584 - val_acc: 0.9874\n",
      "Epoch 48/50\n",
      "14040/14040 [==============================] - 21s 1ms/step - loss: 0.0035 - acc: 0.9991 - val_loss: 0.0584 - val_acc: 0.9876\n",
      "Epoch 49/50\n",
      "14040/14040 [==============================] - 21s 1ms/step - loss: 0.0034 - acc: 0.9992 - val_loss: 0.0572 - val_acc: 0.9878\n",
      "Epoch 50/50\n",
      "14040/14040 [==============================] - 21s 1ms/step - loss: 0.0032 - acc: 0.9993 - val_loss: 0.0594 - val_acc: 0.9872\n"
     ]
    }
   ],
   "source": [
    "#get epoch wise train error and validation error\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, f1_score, classification_report\n",
    "def train_model(train_data, valid_data):\n",
    "    print(train_data.shape)\n",
    "    print(valid_data.shape)\n",
    "    global max_words, word_emb_dim, case_emb_dim, pos_emb_dim, num_classes\n",
    "    total_epochs = 50\n",
    "    batch_size = 149 #equal batches\n",
    "    #batches = createEqualBatches(data)\n",
    "    model = Build_Model(max_words, word_emb_dim, case_emb_dim, pos_emb_dim)\n",
    "    model.compile(loss= \"categorical_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n",
    "    train_label_vectors = get_label_vectors(train_data)\n",
    "    train_word_feats, train_case_feats, train_pos_feats =  get_feature_vectors(train_data)\n",
    "    \n",
    "    valid_label_vectors = get_label_vectors(valid_data)\n",
    "    valid_word_feats, valid_case_feats, valid_pos_feats =  get_feature_vectors(valid_data)\n",
    "    \n",
    "    \n",
    "    #Conversion to numpy arrays (train)\n",
    "    train_word_feats = np.asarray(train_word_feats)\n",
    "    train_case_feats = np.asarray(train_case_feats)\n",
    "    train_pos_feats = np.asarray(train_pos_feats)\n",
    "    train_label_vectors = np.asarray(train_label_vectors)\n",
    "    print(train_word_feats.shape, train_case_feats.shape, train_pos_feats.shape, train_label_vectors.shape)\n",
    "    \n",
    "    #conversion to numpy arrays (test)\n",
    "    valid_word_feats = np.asarray(valid_word_feats)\n",
    "    valid_case_feats = np.asarray(valid_case_feats)\n",
    "    valid_pos_feats = np.asarray(valid_pos_feats)\n",
    "    valid_label_vectors = np.asarray(valid_label_vectors)\n",
    "    print(valid_word_feats.shape, valid_case_feats.shape, valid_pos_feats.shape, valid_label_vectors.shape)\n",
    "    \n",
    "    #valid_label_vectors =  convert_to_scalar(valid_label_vectors)\n",
    "    \n",
    "    \n",
    "    \n",
    "    best = 0.0\n",
    "    model.fit([train_word_feats, train_case_feats, train_pos_feats], train_label_vectors, batch_size = batch_size, validation_data = ([valid_word_feats, valid_case_feats, valid_pos_feats],valid_label_vectors), initial_epoch=0, epochs = total_epochs)\n",
    "    model.save(\"best_model2_ep_50.h5\")\n",
    "train_model(data, valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3249, 4)\n",
      "(3452, 4)\n"
     ]
    }
   ],
   "source": [
    "#Parsing and preprocessing of the training_data\n",
    "valid_data = process_data_file(\"NER-datasets/CONLL2003/valid.txt\")\n",
    "test_data = process_data_file(\"NER-datasets/CONLL2003/test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exlude_padded_class(scalar_label_vectors):\n",
    "    new_scalar_label_vectors = []\n",
    "    for instance in scalar_label_vectors:\n",
    "        temp = []\n",
    "        for label in instance:\n",
    "            if label != 9:\n",
    "                temp.append(label)\n",
    "        new_scalar_label_vectors.append(temp)\n",
    "    new_scalar_label_vectors = np.asarray(new_scalar_label_vectors)\n",
    "    return new_scalar_label_vectors\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  953    31    14    25     0    31    11     3     0     0]\n",
      " [   27 26904    24    10     3    10    16    12     1     0]\n",
      " [   21    40   550     9     0    21     2     2     1     0]\n",
      " [    2     7     3  1335     9     9     2     0     0     0]\n",
      " [    0    15     0     7   874     0     0     0     0     0]\n",
      " [   26     9     6    14     1  1337     1     1     2     0]\n",
      " [    5    32     0     1    13     3   450     4    14     0]\n",
      " [    0    36    11     1     2     1    11   171     4     0]\n",
      " [    0     4     0     0     3     4    10     3   164     0]\n",
      " [    0     0     0     0     0     0     0     0     0 15407]]\n"
     ]
    }
   ],
   "source": [
    "# FIRST RUN ON VALIDATION DATA ##### ###### NOT THE FINAL MODEL ###########\n",
    "from sklearn.metrics import classification_report\n",
    "def metric(model_file, test_data):\n",
    "    model = load_model(model_file)\n",
    "    test_label_vectors = get_label_vectors(test_data)\n",
    "    test_word_feats, test_case_feats, test_pos_feats =  get_feature_vectors(test_data)\n",
    "    predicted_label_vectors = model.predict(([test_word_feats, test_case_feats, test_pos_feats]))\n",
    "    test_label_vectors =  convert_to_scalar(test_label_vectors)\n",
    "    #test_label_vectors = exlude_padded_class(test_label_vectors)\n",
    "    \n",
    "    predicted_label_vectors = convert_to_scalar(predicted_label_vectors)\n",
    "    \n",
    "    #predicted_label_vectors = exlude_padded_class(predicted_label_vectors)\n",
    "    #labels = ['B-ORG', 'O', 'B-MISC', 'B-PER', 'I-PER', 'B-LOC', 'I-ORG', 'I-MISC', 'I-LOC', 'PADDED_LABEL']\n",
    "    \n",
    "    cf = confusion_matrix(test_label_vectors.ravel(), predicted_label_vectors.ravel())\n",
    "    print(cf)\n",
    "#     macro_score = f1_score(test_label_vectors.ravel(), predicted_label_vectors.ravel(), average='macro')\n",
    "#     micro_score = f1_score(test_label_vectors.ravel(), predicted_label_vectors.ravel(), average='micro')\n",
    "#     weighted_score = f1_score(test_label_vectors.ravel(), predicted_label_vectors.ravel(), average='weighted')\n",
    "#     print(macro_score, micro_score, weighted_score)\n",
    "    cf_report = classification_report(test_label_vectors.ravel(), predicted_label_vectors.ravel(), target_names= ['B-ORG', 'O', 'B-MISC', 'B-PER', 'I-PER', 'B-LOC', 'I-ORG', 'I-MISC', 'I-LOC', 'PADDED_LABEL'])\n",
    "    return cf_report\n",
    "\n",
    "model_file = \"best_model2.h5\"\n",
    "cf_report = metric(model_file,valid_data)\n",
    "print(cf_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-ORG       0.92      0.89      0.91      1068\n",
      "           O       0.99      1.00      0.99     27007\n",
      "      B-MISC       0.90      0.85      0.88       646\n",
      "       B-PER       0.95      0.98      0.96      1367\n",
      "       I-PER       0.97      0.98      0.97       896\n",
      "       B-LOC       0.94      0.96      0.95      1397\n",
      "       I-ORG       0.89      0.86      0.88       522\n",
      "      I-MISC       0.87      0.72      0.79       237\n",
      "       I-LOC       0.88      0.87      0.88       188\n",
      "PADDED_LABEL       1.00      1.00      1.00     15407\n",
      "\n",
      " avg / total       0.99      0.99      0.99     48735\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Removing the padded_token class the over_all precision, recall and f-score of the system is as follows:\n",
    "Validation Data\n",
    "Precision: 91.67\n",
    "Recall: 90.12\n",
    "F-score: 91.23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1197    46    47    20     0    48    12     1     1     0]\n",
      " [   36 25767    92    29     2    34    24    51    18     0]\n",
      " [   31    32   411    12     0    21     1     4     0     0]\n",
      " [   23    21     3  1184     8    18     0     0     1     0]\n",
      " [    0     3     0     2   817     0     7     1     3     0]\n",
      " [   46    10    23    10     1  1234     1     1     3     0]\n",
      " [   31    47     2     1    10     5   479    10    23     0]\n",
      " [    0    25     6     1     3     0    14   100     4     0]\n",
      " [    0     7     1     0     1     5     8     0   182     0]\n",
      " [    0     0     0     0     0     0     0     0     0 19458]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-ORG       0.88      0.87      0.88      1372\n",
      "           O       0.99      0.99      0.99     26053\n",
      "      B-MISC       0.70      0.80      0.75       512\n",
      "       B-PER       0.94      0.94      0.94      1258\n",
      "       I-PER       0.97      0.98      0.98       833\n",
      "       B-LOC       0.90      0.93      0.92      1329\n",
      "       I-ORG       0.88      0.79      0.83       608\n",
      "      I-MISC       0.60      0.65      0.62       153\n",
      "       I-LOC       0.77      0.89      0.83       204\n",
      "PADDED_LABEL       1.00      1.00      1.00     19458\n",
      "\n",
      " avg / total       0.98      0.98      0.98     51780\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def metric(model_file, test_data):\n",
    "    model = load_model(model_file)\n",
    "    test_label_vectors = get_label_vectors(test_data)\n",
    "    test_word_feats, test_case_feats, test_pos_feats =  get_feature_vectors(test_data)\n",
    "    predicted_label_vectors = model.predict(([test_word_feats, test_case_feats, test_pos_feats]))\n",
    "    test_label_vectors =  convert_to_scalar(test_label_vectors)\n",
    "    predicted_label_vectors = convert_to_scalar(predicted_label_vectors)\n",
    "    cf = confusion_matrix(test_label_vectors.ravel(), predicted_label_vectors.ravel())\n",
    "    print(cf)\n",
    "#     macro_score = f1_score(test_label_vectors.ravel(), predicted_label_vectors.ravel(), average='macro')\n",
    "#     micro_score = f1_score(test_label_vectors.ravel(), predicted_label_vectors.ravel(), average='micro')\n",
    "#     weighted_score = f1_score(test_label_vectors.ravel(), predicted_label_vectors.ravel(), average='weighted')\n",
    "#     print(macro_score, micro_score, weighted_score)\n",
    "    print(classification_report(test_label_vectors.ravel(), predicted_label_vectors.ravel(), target_names= ['B-ORG', 'O', 'B-MISC', 'B-PER', 'I-PER', 'B-LOC', 'I-ORG', 'I-MISC', 'I-LOC', 'PADDED_LABEL']))\n",
    "\n",
    "model_file = \"best_model2_ep_50.h5\"\n",
    "metric(model_file,test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing the padded_token class the over_all precision, recall and f-score of the system is as follows:\n",
    "Test Data\n",
    "# batch_size = 149, epochs = 30\n",
    "Precision: 84.67\n",
    "Recall: 85.56\n",
    "F1-score: 85\n",
    "\n",
    "# batch size = 256 , epochs = 30\n",
    "Test Data\n",
    "Precision: 84.3\n",
    "Recall: 85.78\n",
    "F1-score: 85.23\n",
    "\n",
    "# batch size = 149, epochs = 50\n",
    "Test Data\n",
    "Precision: 84.78\n",
    "Recall: 87.12\n",
    "F1-score: 86\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collapse_classes(ravel_label_vec):\n",
    "    new_label_vec = []\n",
    "    for label in ravel_label_vec:\n",
    "        if label == 6 or label == 0:\n",
    "            label = 0\n",
    "        elif label == 7 or label == 2:\n",
    "            label = 2\n",
    "        elif label == 4 or label == 3:\n",
    "            label = 3\n",
    "        elif label == 8 or label == 5:\n",
    "            label = 4\n",
    "        elif label == 9:\n",
    "            label = 5\n",
    "        new_label_vec.append(label)\n",
    "    new_label_vec = np.asarray(new_label_vec)\n",
    "    return new_label_vec   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1719    93    60    31    77     0]\n",
      " [   60 25767   143    31    52     0]\n",
      " [   46    57   521    16    25     0]\n",
      " [   30    24     4  2011    22     0]\n",
      " [   55    17    25    12  1424     0]\n",
      " [    0     0     0     0     0 19458]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ORG       0.90      0.87      0.88      1980\n",
      "           O       0.99      0.99      0.99     26053\n",
      "        MISC       0.69      0.78      0.73       665\n",
      "         PER       0.96      0.96      0.96      2091\n",
      "         LOC       0.89      0.93      0.91      1533\n",
      "PADDED_CLASS       1.00      1.00      1.00     19458\n",
      "\n",
      " avg / total       0.98      0.98      0.98     51780\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def metric(model_file, test_data):\n",
    "    model = load_model(model_file)\n",
    "    test_label_vectors = get_label_vectors(test_data)\n",
    "    test_word_feats, test_case_feats, test_pos_feats =  get_feature_vectors(test_data)\n",
    "    predicted_label_vectors = model.predict(([test_word_feats, test_case_feats, test_pos_feats]))\n",
    "    test_label_vectors =  convert_to_scalar(test_label_vectors)\n",
    "    predicted_label_vectors = convert_to_scalar(predicted_label_vectors)\n",
    "    \n",
    "    test_label_vectors = collapse_classes(test_label_vectors.ravel())\n",
    "    predicted_label_vectors = collapse_classes(predicted_label_vectors.ravel())\n",
    "    cf = confusion_matrix(test_label_vectors.ravel(), predicted_label_vectors.ravel())\n",
    "    print(cf)\n",
    "    target_names = ['ORG', 'O', 'MISC', 'PER','LOC', 'PADDED_CLASS']\n",
    "    print(classification_report(test_label_vectors, predicted_label_vectors, target_names = target_names))\n",
    "    return cf\n",
    "\n",
    "model_file = \"best_model2_ep_50.h5\"\n",
    "cf = metric(model_file,test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for 50 epochs and a batch size of 149 (Overall System Precision, Recall and F1-score)\n",
    "Test Data\n",
    "Precision: 84.78\n",
    "Recall: 87.12\n",
    "F1-score: 86"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot a confusion Matrix\n",
    "from matplotlib import pyplot as plt\n",
    "labels = ['B-ORG', 'O', 'B-MISC', 'B-PER','B-LOC', 'PADDED_CLASS']\n",
    "#cm = confusion_matrix(y_test, pred, labels)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(cf)\n",
    "plt.title('Confusion matrix of the classifier')\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticklabels([''] + labels)\n",
    "ax.set_yticklabels([''] + labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
