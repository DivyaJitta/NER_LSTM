{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parsing and preprocessing of the training_data\n",
    "import numpy as np\n",
    "import string\n",
    "def process_data_file(file_name):\n",
    "    f = open(file_name, \"r\")\n",
    "    tokens = []\n",
    "    pos = []\n",
    "    chunks = []\n",
    "    ner_lables = []\n",
    "    data = []\n",
    "    for line in  f:\n",
    "        if line == \"\\n\" or line.startswith('-DOCSTART'):\n",
    "            if len(tokens) > 0:\n",
    "                sent = []\n",
    "                sent.append(tokens)\n",
    "                sent.append(pos)\n",
    "                sent.append(chunks)\n",
    "                sent.append(ner_lables)\n",
    "                data.append(sent)\n",
    "\n",
    "                # refresh the arrays\n",
    "                tokens = []\n",
    "                pos = []\n",
    "                chunks = []\n",
    "                ner_lables = []\n",
    "\n",
    "        else:\n",
    "            try:\n",
    "                parts = line.strip(\"\\n\").split(\" \")\n",
    "                tokens.append(parts[0])\n",
    "                pos.append(parts[1])\n",
    "                chunks.append(parts[2])\n",
    "                ner_lables.append(parts[3])\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "    #data = data[1:]\n",
    "    data = np.asarray(data)\n",
    "    print(data.shape)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14040, 4)\n",
      "(3249, 4)\n",
      "(3452, 4)\n"
     ]
    }
   ],
   "source": [
    "data = process_data_file(\"NER-datasets/CONLL2003/train.txt\")\n",
    "valid_data = process_data_file(\"NER-datasets/CONLL2003/valid.txt\")\n",
    "test_data = process_data_file(\"NER-datasets/CONLL2003/test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['BRUSSELS', '1996-08-22']), list(['NNP', 'CD']),\n",
       "       list(['B-NP', 'I-NP']), list(['B-LOC', 'O'])], dtype=object)"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createEqualBatches(data):\n",
    "    n_batches = 100\n",
    "    batch_size = len(data) // n_batches\n",
    "    #print(batch_size)\n",
    "    indices = []\n",
    "    for i in range(n_batches):\n",
    "        indices.append(batch_size*(i+1))\n",
    "    #print(indices)\n",
    "    \n",
    "    batches = []\n",
    "    batch_len = []\n",
    "    z = 0\n",
    "    start = 0\n",
    "    print(len(indices))\n",
    "    for end in indices:\n",
    "        #print(\"start, end\", start, end)\n",
    "        batches.append(data[start:end])\n",
    "        start = end\n",
    "\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "batches = createEqualBatches(data)\n",
    "print(len(batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[list(['Peter', 'Blackburn']) list(['NNP', 'NNP']) list(['B-NP', 'I-NP'])\n",
      " list(['B-PER', 'I-PER'])]\n"
     ]
    }
   ],
   "source": [
    "print(data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Intializing some global vars\n",
    "Embeddings = {}\n",
    "word_emb_dim  = 50\n",
    "embedding_file = \"embeddings/glove.6B.50d.txt\"\n",
    "def LoadEmbeddings(embedding_file):\n",
    "    global Embeddings, word_emb_dim\n",
    "    f = open(embedding_file, \"r\", encoding= \"utf-8\", errors= \"ignore\")\n",
    "    for line in f :\n",
    "        tokens = line.strip(\"\\n\").split()\n",
    "        word = tokens[0].lower()\n",
    "        vec = tokens[1:]\n",
    "        vec = \" \". join(vec)\n",
    "        Embeddings[word] = vec\n",
    "    Embeddings[\"zero_vec\"] = \"0.0 \" * word_emb_dim\n",
    "    Embeddings[\"zero_vec\"] = Embeddings[\"zero_vec\"].rstrip()\n",
    "    f.close()\n",
    "    \n",
    "    g = open (\"embeddings/EmbedDict.pkl\", \"wb\")\n",
    "    pickle.dump(Embeddings, g)\n",
    "    g.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "LoadEmbeddings(\"embeddings/glove.6B.50d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "GloveEmbeddings = {}\n",
    "pos_emebddings = {}\n",
    "max_words = 15\n",
    "case_emb_dim = 8\n",
    "pos_emb_dim = 40\n",
    "num_classes =  10\n",
    "def load_word_embeddings():\n",
    "    #load the embeddings from the pickle file\n",
    "    global GloveEmbeddings\n",
    "    f = open(\"embeddings/EmbedDict.pkl\", \"rb\")\n",
    "    GloveEmbeddings = pickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "def get_word_feat_vecs(words, left_over):\n",
    "    global max_words, GloveEmbeddings\n",
    "    #remove extra words\n",
    "    #print(words, left_over)\n",
    "    if left_over < 0:\n",
    "        words = words[:max_words]\n",
    "    elif(left_over > 0): #padding req\n",
    "        for i in range(left_over):\n",
    "            words.append(\"zero_vec\")\n",
    "    \n",
    "    # now obtain the feature vector\n",
    "    feat_vec = []\n",
    "    for word in words:\n",
    "        word = word.lower()\n",
    "        try:\n",
    "            glove_vec = []\n",
    "            splits = GloveEmbeddings[word].split()\n",
    "            #print(splits)\n",
    "            for v in splits:\n",
    "                glove_vec.append(float(v))\n",
    "            #glove_vec = [float(v for v in GloveEmbeddings[word].split())]\n",
    "        except:\n",
    "            glove_vec = []\n",
    "            splits = GloveEmbeddings[\"zero_vec\"].split()\n",
    "            #print(splits)\n",
    "            for v in splits:\n",
    "                glove_vec.append(float(v))\n",
    "        feat_vec.append(glove_vec)\n",
    "    return feat_vec\n",
    "\n",
    "\n",
    "        \n",
    "def  get_case_embeddings(word):\n",
    "    global case_emb_dim\n",
    "    case_map = {'numeric': 0, 'all_lower': 1, 'all_upper': 2, 'initial_upper': 3, 'other': 4, 'mainly_numeric': 5,\n",
    "                    'contains_digit': 6, 'PADDED_TOKEN': 7}\n",
    "    num_of_digits = 0\n",
    "    for char in word:\n",
    "        if char.isdigit():\n",
    "            num_of_digits += 1\n",
    "    number_part = float(num_of_digits/len(word))\n",
    "    \n",
    "    casing = 'other'\n",
    "    \n",
    "    if num_of_digits == len(word):\n",
    "        casing = \"numeric\"\n",
    "    elif number_part >= 0.5:\n",
    "        casing = \"mainly_numeric\"\n",
    "    elif num_of_digits > 0:\n",
    "        casing = \"contains_digit\"\n",
    "    elif word.isupper():\n",
    "        casing = \"all_upper\"\n",
    "    elif word.islower():\n",
    "        casing = \"all_lower\"\n",
    "    elif word[0].lower():\n",
    "        casing = \"initial_upper\"\n",
    "    elif word[0].lower() == \"zero_vec\":\n",
    "        casing = \"PADDING_TOKEN\"\n",
    "    \n",
    "    case_vector = [0]*case_emb_dim\n",
    "    case_vector[case_map[casing]] = 1\n",
    "    \n",
    "    return case_vector\n",
    "\n",
    "###################### Create pickle files for  pos and label embeddings #####################\n",
    "\n",
    "def create_pos_embeddings(data):\n",
    "    print(len(data))\n",
    "    pos_statistics = {}\n",
    "    pos_set = []\n",
    "    for instance in data:\n",
    "        pos_tags = instance[1]\n",
    "        #print(pos_tags)\n",
    "        for tag in pos_tags:\n",
    "            if tag in string.punctuation:\n",
    "                tag = \"PUNCT\"\n",
    "            try:\n",
    "                pos_statistics[tag] = pos_statistics[tag] + 1\n",
    "            except:\n",
    "                pos_statistics[tag] = 1\n",
    "    \n",
    "    print(pos_statistics)\n",
    "    for pos in pos_statistics:\n",
    "        pos_set.append(pos)\n",
    "    pos_set.append(\"PADDED_POS_TAG\")\n",
    "    \n",
    "    print(pos_set)\n",
    "    print(len(pos_set))\n",
    "    pos_map = {}\n",
    "    for pos_idx in range(len(pos_set)):\n",
    "        pos_vec = [0]* len(pos_set)\n",
    "        pos_vec[pos_idx] = 1\n",
    "        pos_map[pos_set[pos_idx]] = pos_vec\n",
    "    \n",
    "    f = open(\"POSEmbeddings.pickle\", \"wb\")\n",
    "    pickle.dump(pos_map, f)\n",
    "    f.close()\n",
    "    \n",
    "def create_label_embeddings(data):\n",
    "    label_set = []\n",
    "    label_statistics = {}\n",
    "    for instance in data:\n",
    "        labels = instance[3]\n",
    "        for label in labels:\n",
    "            try:\n",
    "                label_statistics[label] = label_statistics[label] + 1\n",
    "            except:\n",
    "                label_statistics[label] = 1\n",
    "    ################ COMMENT WHEN LABEL STATISTICS IS NOT NEEDED ##################\n",
    "#     print(label_statistics)\n",
    "#     total_count = 0\n",
    "#     for label in label_statistics:\n",
    "#         total_count += label_statistics[label]\n",
    "    \n",
    "#     for label, count in label_statistics.items():\n",
    "#         print(\"{}: {}%\".format(label, round((count/total_count)*100, 2)))\n",
    "    ######################################################################################\n",
    "    for label in label_statistics:\n",
    "        label_set.append(label)\n",
    "    label_set.append(\"PADDED_LABEL\")\n",
    "    print(label_set)\n",
    "    print(len(label_set))\n",
    "    \n",
    "    label_map = {}\n",
    "    \n",
    "    for label_idx in range(len(label_set)):\n",
    "        label_vec = [0] * len(label_set)\n",
    "        label_vec[label_idx] = 1\n",
    "        label_map[label_set[label_idx]] = label_vec\n",
    "    \n",
    "    f = open(\"LabelEmbed.pickle\", \"wb\")\n",
    "    pickle.dump(label_map, f)\n",
    "    f.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B-ORG': 6319, 'O': 169576, 'B-MISC': 3438, 'B-PER': 6600, 'I-PER': 4528, 'B-LOC': 7140, 'I-ORG': 3704, 'I-MISC': 1155, 'I-LOC': 1157}\n",
      "B-ORG: 3.1%\n",
      "O: 83.28%\n",
      "B-MISC: 1.69%\n",
      "B-PER: 3.24%\n",
      "I-PER: 2.22%\n",
      "B-LOC: 3.51%\n",
      "I-ORG: 1.82%\n",
      "I-MISC: 0.57%\n",
      "I-LOC: 0.57%\n",
      "\n",
      "\n",
      "{'O': 42757, 'B-ORG': 1340, 'B-LOC': 1837, 'B-MISC': 922, 'I-MISC': 346, 'B-PER': 1842, 'I-PER': 1307, 'I-LOC': 257, 'I-ORG': 750}\n",
      "O: 83.25%\n",
      "B-ORG: 2.61%\n",
      "B-LOC: 3.58%\n",
      "B-MISC: 1.8%\n",
      "I-MISC: 0.67%\n",
      "B-PER: 3.59%\n",
      "I-PER: 2.54%\n",
      "I-LOC: 0.5%\n",
      "I-ORG: 1.46%\n",
      "\n",
      "\n",
      "{'O': 38289, 'B-LOC': 1667, 'B-PER': 1616, 'I-PER': 1156, 'I-LOC': 257, 'B-MISC': 701, 'I-MISC': 214, 'B-ORG': 1660, 'I-ORG': 834}\n",
      "O: 82.53%\n",
      "B-LOC: 3.59%\n",
      "B-PER: 3.48%\n",
      "I-PER: 2.49%\n",
      "I-LOC: 0.55%\n",
      "B-MISC: 1.51%\n",
      "I-MISC: 0.46%\n",
      "B-ORG: 3.58%\n",
      "I-ORG: 1.8%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "create_label_embeddings(data)\n",
    "print(\"\\n\")\n",
    "create_label_embeddings(valid_data)\n",
    "print(\"\\n\")\n",
    "create_label_embeddings(test_data)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-ORG', 'O', 'B-MISC', 'B-PER', 'I-PER', 'B-LOC', 'I-ORG', 'I-MISC', 'I-LOC', 'PADDED_LABEL']\n",
      "10\n",
      "14040\n",
      "{'NNP': 34391, 'VBZ': 2426, 'JJ': 11831, 'NN': 23898, 'TO': 3469, 'VB': 4252, 'PUNCT': 25403, 'CD': 19702, 'DT': 13453, 'VBD': 8293, 'IN': 19064, 'PRP': 3163, 'NNS': 9903, 'VBP': 1436, 'MD': 1199, 'VBN': 4105, 'POS': 1553, 'JJR': 382, 'RB': 3975, 'FW': 166, 'CC': 3653, 'WDT': 506, 'PRP$': 1520, 'RBR': 163, 'VBG': 2585, 'EX': 136, 'WP': 528, 'WRB': 384, 'RP': 528, 'NNPS': 684, 'SYM': 439, 'RBS': 35, 'UH': 30, 'PDT': 33, \"''\": 35, 'LS': 13, 'JJS': 254, 'WP$': 23, 'NN|SYM': 4}\n",
      "['NNP', 'VBZ', 'JJ', 'NN', 'TO', 'VB', 'PUNCT', 'CD', 'DT', 'VBD', 'IN', 'PRP', 'NNS', 'VBP', 'MD', 'VBN', 'POS', 'JJR', 'RB', 'FW', 'CC', 'WDT', 'PRP$', 'RBR', 'VBG', 'EX', 'WP', 'WRB', 'RP', 'NNPS', 'SYM', 'RBS', 'UH', 'PDT', \"''\", 'LS', 'JJS', 'WP$', 'NN|SYM', 'PADDED_POS_TAG']\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "create_label_embeddings(data) # Label Statistics\n",
    "create_pos_embeddings(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.502635327635328\n",
      "{9: 894, 2: 1129, 30: 194, 31: 209, 33: 173, 25: 223, 40: 80, 28: 217, 37: 102, 27: 202, 1: 179, 26: 212, 35: 135, 39: 109, 34: 155, 15: 279, 16: 235, 10: 503, 32: 186, 12: 324, 8: 1056, 47: 21, 24: 221, 44: 45, 42: 64, 11: 407, 19: 195, 21: 213, 14: 293, 17: 233, 29: 216, 22: 217, 3: 586, 7: 904, 13: 354, 5: 751, 20: 214, 6: 625, 46: 27, 38: 98, 36: 124, 18: 198, 4: 770, 52: 11, 23: 210, 41: 72, 50: 15, 45: 40, 54: 4, 43: 52, 48: 18, 51: 10, 58: 2, 57: 3, 60: 2, 49: 13, 53: 3, 55: 5, 62: 2, 78: 1, 56: 1, 59: 2, 113: 1, 67: 1}\n"
     ]
    }
   ],
   "source": [
    "def average_sent_len(data):\n",
    "    word_lens = []\n",
    "    len_dict ={}\n",
    "    for instance in data:\n",
    "        words = instance[0]\n",
    "        word_lens.append(len(words))\n",
    "    print(float(sum(word_lens)/len(word_lens)))\n",
    "    for length in word_lens:\n",
    "        try:\n",
    "            len_dict[length] += 1\n",
    "        except:\n",
    "            len_dict[length] = 1\n",
    "    print(len_dict)\n",
    "\n",
    "average_sent_len(data) #Average length of the snetences\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remaining(words):\n",
    "    global max_words\n",
    "    word_count = len(words)\n",
    "    left_over = max_words - word_count\n",
    "    return left_over\n",
    "\n",
    "def get_label_vectors(data):\n",
    "    global num_classes, max_words\n",
    "    data_label_vectors = []\n",
    "    f = open(\"LabelEmbed.pickle\", \"rb\")\n",
    "    label_map = pickle.load(f)\n",
    "    f.close()\n",
    "    \n",
    "    for  instance in data:\n",
    "        labels = instance[3]\n",
    "        left_over = remaining(labels)\n",
    "        if left_over < 0:\n",
    "            #print(\"no pad\")\n",
    "            labels =  labels[:max_words]\n",
    "        temp = []\n",
    "        for label in labels:\n",
    "            label_vec = label_map[label]\n",
    "            temp.append(label_vec)\n",
    "        if left_over > 0:\n",
    "            #print(\"Padded\")\n",
    "            label_vec = [0] * num_classes\n",
    "            label_vec[9] = 1\n",
    "            for i in range(left_over):\n",
    "                temp.append(label_vec)\n",
    "        #print(len(temp))\n",
    "        #print(np.asarray(temp).shape)\n",
    "        data_label_vectors.append(temp)\n",
    "    \n",
    "    return data_label_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14040, 15, 10)\n"
     ]
    }
   ],
   "source": [
    "data_label_vecs = get_label_vectors(data)\n",
    "a = np.asarray(data_label_vecs)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_case_feat_vecs(words, left_over):\n",
    "    global case_emb_dim, max_words\n",
    "    case_vecs = []\n",
    "    if left_over < 0:\n",
    "        words = words[:max_words]\n",
    "    for word in words:\n",
    "        case_embedding = get_case_embeddings(word)\n",
    "        case_vecs.append(case_embedding)\n",
    "    if left_over > 0:\n",
    "        case_embedding = [0] * case_emb_dim\n",
    "        case_embedding[7] = 1\n",
    "        for i in range(left_over):\n",
    "            case_vecs.append(case_embedding)\n",
    "    return case_vecs\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_case_feat_vecs() missing 1 required positional argument: 'left_over'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-85-8ca169b2e8af>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_case_feat_vecs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: get_case_feat_vecs() missing 1 required positional argument: 'left_over'"
     ]
    }
   ],
   "source": [
    "print(len(get_case_feat_vecs(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pos_embeddings():\n",
    "    global pos_embeddings\n",
    "    f = open(\"POSEmbeddings.pickle\", \"rb\")\n",
    "    pos_embeddings = pickle.load(f)\n",
    "    f.close()\n",
    "    \n",
    "def get_pos_feat_vecs(pos, left_over):\n",
    "    global pos_embeddings, pos_emb_dim, max_words\n",
    "    pos_vecs = []\n",
    "    if left_over < 0:\n",
    "        pos = pos[:max_words]\n",
    "    for tag in pos:\n",
    "        if tag in string.punctuation:\n",
    "            tag = \"PUNCT\"\n",
    "        pos_vec = pos_embeddings[tag]\n",
    "        pos_vecs.append(pos_vec)\n",
    "    if left_over > 0:\n",
    "        pos_vec = pos_embeddings[\"PADDED_POS_TAG\"]\n",
    "        for i in range(left_over):\n",
    "            pos_vecs.append(pos_vec)\n",
    "    return pos_vecs\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_pos_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_vectors(data):\n",
    "    ## This function gets the feature vectors (word, case and pos) for the entire training data ###\n",
    "    \n",
    "    # get feats for words\n",
    "    global max_words\n",
    "    data_word_feats = []\n",
    "    data_case_feats = []\n",
    "    data_pos_feats = []\n",
    "    load_word_embeddings()\n",
    "    load_pos_embeddings()\n",
    "    for instance in data:\n",
    "        tokens = instance[0]\n",
    "        pos = instance[1]\n",
    "#         if (len(tokens) != len(pos)):\n",
    "#             print(\"WRONG\")\n",
    "#             print(tokens)\n",
    "#             print(pos)\n",
    "        left_over = remaining(tokens)\n",
    "        \n",
    "       \n",
    "        #get feats for POS\n",
    "        pos_feat_vecs = get_pos_feat_vecs(pos, left_over)\n",
    "        data_pos_feats.append(pos_feat_vecs)\n",
    "        #get feats for case\n",
    "        case_feat_vecs = get_case_feat_vecs(tokens, left_over)\n",
    "        data_case_feats.append(case_feat_vecs)\n",
    "        #get for words\n",
    "        words_feat_vecs = get_word_feat_vecs(tokens, left_over)\n",
    "        data_word_feats.append(words_feat_vecs)\n",
    "        \n",
    "       \n",
    "    \n",
    "    return data_word_feats, data_case_feats, data_pos_feats\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model, load_model, save_model\n",
    "from keras.layers import Bidirectional, LSTM, Dense, Activation, Input, concatenate, TimeDistributed\n",
    "def Build_Model(sent_max_words = 15, word_emb_dim = 50 , case_emb_dim = 7, pos_emb_dim = 46):\n",
    "    lstm_dim = 200\n",
    "    global num_classes\n",
    "    #word input\n",
    "    word_input = Input(shape=(sent_max_words, word_emb_dim))\n",
    "    #case input\n",
    "    case_input = Input(shape=(sent_max_words, case_emb_dim))\n",
    "    #pos_input\n",
    "    pos_input = Input(shape=(sent_max_words, pos_emb_dim))\n",
    "    #Concatenate the three inputs\n",
    "    merged_input = concatenate([word_input, case_input, pos_input])\n",
    "    #pass the merged input to a BiLSTM\n",
    "    lstm_output = Bidirectional(LSTM(lstm_dim, return_sequences=True, dropout = 0.2),merge_mode=None)(merged_input)\n",
    "    merged_output = concatenate([lstm_output[0], lstm_output[1]], axis = 2)\n",
    "    #Add a Time Distributed Layer\n",
    "    output = TimeDistributed(Dense(num_classes, activation = \"softmax\"), name = \"Softmax_Layer\")(merged_output)\n",
    "    \n",
    "    ## Model ##\n",
    "    model = Model(inputs = [word_input, case_input, pos_input], outputs = output)\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 15, 50)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 15, 7)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 15, 46)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 15, 103)      0           input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) [(None, 15, 200), (N 486400      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 15, 400)      0           bidirectional_1[0][0]            \n",
      "                                                                 bidirectional_1[0][1]            \n",
      "__________________________________________________________________________________________________\n",
      "Softmax_Layer (TimeDistributed) (None, 15, 10)       4010        concatenate_2[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 490,410\n",
      "Trainable params: 490,410\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.training.Model at 0x19491065c50>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Build_Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14040, 4)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_22 (InputLayer)           (None, 15, 50)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_23 (InputLayer)           (None, 15, 7)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_24 (InputLayer)           (None, 15, 39)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)    (None, 15, 96)       0           input_22[0][0]                   \n",
      "                                                                 input_23[0][0]                   \n",
      "                                                                 input_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_8 (Bidirectional) [(None, 15, 200), (N 475200      concatenate_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_16 (Concatenate)    (None, 15, 400)      0           bidirectional_8[0][0]            \n",
      "                                                                 bidirectional_8[0][1]            \n",
      "__________________________________________________________________________________________________\n",
      "Softmax_Layer (TimeDistributed) (None, 15, 10)       4010        concatenate_16[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 479,210\n",
      "Trainable params: 479,210\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "10\n",
      "14040\n",
      "(14040, 15, 50)\n",
      "(14040, 15, 7)\n",
      "(14040, 15, 39)\n",
      "(14040, 15)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected Softmax_Layer to have 3 dimensions, but got array with shape (14040, 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-108-8d385715133d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-108-8d385715133d>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos_feats\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_label_vectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword_feats\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcase_feats\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos_feats\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_label_vectors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m149\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtotal_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;31m#     for epoch in  range(total_epochs):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;31m#         model.fit([word_feats, case_feats, pos_feats], train_label_vectors, batch_size = 149, validation_split = 0)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\z003ws3e\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 952\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m    953\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\z003ws3e\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 789\u001b[1;33m                 exception_prefix='target')\n\u001b[0m\u001b[0;32m    790\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m             \u001b[1;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\z003ws3e\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    126\u001b[0m                         \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' dimensions, but got array '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[0;32m    129\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking target: expected Softmax_Layer to have 3 dimensions, but got array with shape (14040, 15)"
     ]
    }
   ],
   "source": [
    "# Do not RUN #\n",
    "#get epoch wise train error and validation error\n",
    "import numpy as np\n",
    "def train_model(data):\n",
    "    print(data.shape)\n",
    "    global max_words, word_emb_dim, case_emb_dim, pos_emb_dim, num_classes\n",
    "    total_epochs = 30\n",
    "    batch_size = 149 #equal batches\n",
    "    #batches = createEqualBatches(data)\n",
    "    model = Build_Model(max_words, word_emb_dim, case_emb_dim, pos_emb_dim)\n",
    "    model.compile(loss= \"categorical_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n",
    "    train_label_vectors = get_label_vectors(data)\n",
    "    word_feats, case_feats, pos_feats =  get_feature_vectors(data)\n",
    "    #print(len(word_feats), len(case_feats), len(pos_feats))\n",
    "    \n",
    "    print(len(train_label_vectors))\n",
    "    \n",
    "    #Metrics for each epoch\n",
    "    word_feats = np.asarray(word_feats)\n",
    "    case_feats = np.asarray(case_feats)\n",
    "    pos_feats = np.asarray(pos_feats)\n",
    "    train_label_vectors = np.asarray(train_label_vectors)\n",
    "    print(word_feats.shape)\n",
    "    print(case_feats.shape)\n",
    "    print(pos_feats.shape)\n",
    "    print(train_label_vectors.shape)\n",
    "    model.fit([word_feats, case_feats, pos_feats], train_label_vectors, batch_size = 149, validation_split = 0.2, initial_epoch=0, epochs = total_epochs)\n",
    "#     for epoch in  range(total_epochs):\n",
    "#         model.fit([word_feats, case_feats, pos_feats], train_label_vectors, batch_size = 149, validation_split = 0)\n",
    "\n",
    "        \n",
    "\n",
    "train_model(data)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3249, 4)\n"
     ]
    }
   ],
   "source": [
    "#Parsing and preprocessing of the validation_data\n",
    "valid_data = process_data_file(\"NER-datasets/CONLL2003/valid.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_scalar(label_vectors):\n",
    "    scalar_vec = []\n",
    "    for sentence in label_vectors:\n",
    "        labels = []\n",
    "        for word in sentence:\n",
    "            #print(word)\n",
    "            idx = np.argmax(word)\n",
    "            labels.append(idx)\n",
    "        scalar_vec.append(labels)\n",
    "    scalar_vec = np.asarray(scalar_vec)\n",
    "    \n",
    "    return scalar_vec\n",
    "\n",
    "#  try:\n",
    "#                 idx = np.where(word==1)\n",
    "#                 idx = int(idx[0])\n",
    "#             except:\n",
    "#                 idx = np.argmax(word)\n",
    "#                 idx = int(idx)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14040, 4)\n",
      "(3249, 4)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_22 (InputLayer)           (None, 15, 50)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_23 (InputLayer)           (None, 15, 8)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_24 (InputLayer)           (None, 15, 40)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)    (None, 15, 98)       0           input_22[0][0]                   \n",
      "                                                                 input_23[0][0]                   \n",
      "                                                                 input_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_8 (Bidirectional) [(None, 15, 200), (N 478400      concatenate_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_16 (Concatenate)    (None, 15, 400)      0           bidirectional_8[0][0]            \n",
      "                                                                 bidirectional_8[0][1]            \n",
      "__________________________________________________________________________________________________\n",
      "Softmax_Layer (TimeDistributed) (None, 15, 10)       4010        concatenate_16[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 482,410\n",
      "Trainable params: 482,410\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "(14040, 15, 50) (14040, 15, 8) (14040, 15, 40) (14040, 15, 10)\n",
      "(3249, 15, 50) (3249, 15, 8) (3249, 15, 40) (3249, 15, 10)\n",
      "Train on 14040 samples, validate on 3249 samples\n",
      "Epoch 1/30\n",
      "14040/14040 [==============================] - 24s 2ms/step - loss: 0.5429 - acc: 0.8581 - val_loss: 0.2392 - val_acc: 0.9365\n",
      "Epoch 2/30\n",
      "14040/14040 [==============================] - 21s 1ms/step - loss: 0.1732 - acc: 0.9512 - val_loss: 0.1261 - val_acc: 0.9641\n",
      "Epoch 3/30\n",
      "14040/14040 [==============================] - 20s 1ms/step - loss: 0.1137 - acc: 0.9676 - val_loss: 0.0967 - val_acc: 0.9725\n",
      "Epoch 4/30\n",
      "14040/14040 [==============================] - 20s 1ms/step - loss: 0.0928 - acc: 0.9733 - val_loss: 0.0855 - val_acc: 0.9750\n",
      "Epoch 5/30\n",
      "14040/14040 [==============================] - 21s 1ms/step - loss: 0.0807 - acc: 0.9764 - val_loss: 0.0760 - val_acc: 0.9776\n",
      "Epoch 6/30\n",
      "14040/14040 [==============================] - 20s 1ms/step - loss: 0.0709 - acc: 0.9792 - val_loss: 0.0686 - val_acc: 0.9803\n",
      "Epoch 7/30\n",
      "14040/14040 [==============================] - 20s 1ms/step - loss: 0.0636 - acc: 0.9814 - val_loss: 0.0644 - val_acc: 0.9814\n",
      "Epoch 8/30\n",
      "14040/14040 [==============================] - 21s 1ms/step - loss: 0.0579 - acc: 0.9829 - val_loss: 0.0627 - val_acc: 0.9818\n",
      "Epoch 9/30\n",
      "14040/14040 [==============================] - 20s 1ms/step - loss: 0.0530 - acc: 0.9846 - val_loss: 0.0579 - val_acc: 0.9825\n",
      "Epoch 10/30\n",
      "14040/14040 [==============================] - 20s 1ms/step - loss: 0.0483 - acc: 0.9856 - val_loss: 0.0539 - val_acc: 0.9840\n",
      "Epoch 11/30\n",
      "14040/14040 [==============================] - 21s 1ms/step - loss: 0.0448 - acc: 0.9868 - val_loss: 0.0523 - val_acc: 0.9847\n",
      "Epoch 12/30\n",
      "14040/14040 [==============================] - 21s 1ms/step - loss: 0.0408 - acc: 0.9879 - val_loss: 0.0502 - val_acc: 0.9852\n",
      "Epoch 13/30\n",
      "14040/14040 [==============================] - 21s 2ms/step - loss: 0.0379 - acc: 0.9888 - val_loss: 0.0487 - val_acc: 0.9860\n",
      "Epoch 14/30\n",
      "14040/14040 [==============================] - 21s 1ms/step - loss: 0.0348 - acc: 0.9896 - val_loss: 0.0495 - val_acc: 0.9854\n",
      "Epoch 15/30\n",
      "14040/14040 [==============================] - 21s 1ms/step - loss: 0.0319 - acc: 0.9907 - val_loss: 0.0483 - val_acc: 0.9859\n",
      "Epoch 16/30\n",
      "14040/14040 [==============================] - 22s 2ms/step - loss: 0.0293 - acc: 0.9913 - val_loss: 0.0500 - val_acc: 0.9855\n",
      "Epoch 17/30\n",
      "14040/14040 [==============================] - 21s 2ms/step - loss: 0.0271 - acc: 0.9921 - val_loss: 0.0482 - val_acc: 0.9862\n",
      "Epoch 18/30\n",
      "14040/14040 [==============================] - 20s 1ms/step - loss: 0.0253 - acc: 0.9925 - val_loss: 0.0477 - val_acc: 0.9869\n",
      "Epoch 19/30\n",
      "14040/14040 [==============================] - 21s 1ms/step - loss: 0.0234 - acc: 0.9932 - val_loss: 0.0489 - val_acc: 0.9859\n",
      "Epoch 20/30\n",
      "14040/14040 [==============================] - 20s 1ms/step - loss: 0.0213 - acc: 0.9939 - val_loss: 0.0465 - val_acc: 0.9871\n",
      "Epoch 21/30\n",
      "14040/14040 [==============================] - 21s 1ms/step - loss: 0.0200 - acc: 0.9941 - val_loss: 0.0484 - val_acc: 0.9867\n",
      "Epoch 22/30\n",
      "14040/14040 [==============================] - 21s 1ms/step - loss: 0.0187 - acc: 0.9945 - val_loss: 0.0484 - val_acc: 0.9868\n",
      "Epoch 23/30\n",
      "14040/14040 [==============================] - 20s 1ms/step - loss: 0.0173 - acc: 0.9950 - val_loss: 0.0488 - val_acc: 0.9866\n",
      "Epoch 24/30\n",
      "14040/14040 [==============================] - 21s 1ms/step - loss: 0.0156 - acc: 0.9956 - val_loss: 0.0482 - val_acc: 0.9869\n",
      "Epoch 25/30\n",
      "14040/14040 [==============================] - 20s 1ms/step - loss: 0.0140 - acc: 0.9961 - val_loss: 0.0511 - val_acc: 0.9868\n",
      "Epoch 26/30\n",
      "14040/14040 [==============================] - 21s 2ms/step - loss: 0.0133 - acc: 0.9961 - val_loss: 0.0487 - val_acc: 0.9873\n",
      "Epoch 27/30\n",
      "14040/14040 [==============================] - 22s 2ms/step - loss: 0.0124 - acc: 0.9966 - val_loss: 0.0486 - val_acc: 0.9871\n",
      "Epoch 28/30\n",
      "14040/14040 [==============================] - 22s 2ms/step - loss: 0.0114 - acc: 0.9970 - val_loss: 0.0489 - val_acc: 0.9878\n",
      "Epoch 29/30\n",
      "14040/14040 [==============================] - 20s 1ms/step - loss: 0.0109 - acc: 0.9970 - val_loss: 0.0524 - val_acc: 0.9872\n",
      "Epoch 30/30\n",
      "14040/14040 [==============================] - 21s 1ms/step - loss: 0.0099 - acc: 0.9973 - val_loss: 0.0497 - val_acc: 0.9879\n"
     ]
    }
   ],
   "source": [
    "#get epoch wise train error and validation error\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, f1_score, classification_report\n",
    "def train_model(train_data, valid_data):\n",
    "    print(train_data.shape)\n",
    "    print(valid_data.shape)\n",
    "    global max_words, word_emb_dim, case_emb_dim, pos_emb_dim, num_classes\n",
    "    total_epochs = 30\n",
    "    batch_size = 149 #equal batches\n",
    "    #batches = createEqualBatches(data)\n",
    "    model = Build_Model(max_words, word_emb_dim, case_emb_dim, pos_emb_dim)\n",
    "    model.compile(loss= \"categorical_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n",
    "    train_label_vectors = get_label_vectors(train_data)\n",
    "    train_word_feats, train_case_feats, train_pos_feats =  get_feature_vectors(train_data)\n",
    "    \n",
    "    valid_label_vectors = get_label_vectors(valid_data)\n",
    "    valid_word_feats, valid_case_feats, valid_pos_feats =  get_feature_vectors(valid_data)\n",
    "    \n",
    "    \n",
    "    #Conversion to numpy arrays (train)\n",
    "    train_word_feats = np.asarray(train_word_feats)\n",
    "    train_case_feats = np.asarray(train_case_feats)\n",
    "    train_pos_feats = np.asarray(train_pos_feats)\n",
    "    train_label_vectors = np.asarray(train_label_vectors)\n",
    "    print(train_word_feats.shape, train_case_feats.shape, train_pos_feats.shape, train_label_vectors.shape)\n",
    "    \n",
    "    #conversion to numpy arrays (test)\n",
    "    valid_word_feats = np.asarray(valid_word_feats)\n",
    "    valid_case_feats = np.asarray(valid_case_feats)\n",
    "    valid_pos_feats = np.asarray(valid_pos_feats)\n",
    "    valid_label_vectors = np.asarray(valid_label_vectors)\n",
    "    print(valid_word_feats.shape, valid_case_feats.shape, valid_pos_feats.shape, valid_label_vectors.shape)\n",
    "    \n",
    "    #valid_label_vectors =  convert_to_scalar(valid_label_vectors)\n",
    "    \n",
    "    \n",
    "    \n",
    "    best = 0.0\n",
    "    model.fit([train_word_feats, train_case_feats, train_pos_feats], train_label_vectors, batch_size = 149, validation_data = ([valid_word_feats, valid_case_feats, valid_pos_feats],valid_label_vectors), initial_epoch=0, epochs = total_epochs)\n",
    "    model.save(\"best_model2.h5\")\n",
    "#     for epoch in range(total_epochs):\n",
    "#         model.fit([train_word_feats, train_case_feats, train_pos_feats], train_label_vectors, batch_size = 149, validation_split = 0, epochs = 1)\n",
    "#         predicted_label_vectors = model.predict(([valid_word_feats, valid_case_feats, valid_pos_feats]))\n",
    "# #         print(train_label_vectors.shape)\n",
    "# #         print(predicted_label_vectors.shape)\n",
    "#         valid_label_vectors =  convert_to_scalar(valid_label_vectors)\n",
    "#         predicted_label_vectors = convert_to_scalar(predicted_label_vectors)\n",
    "# #         print(valid_label_vectors[0])\n",
    "# #         print(predicted_label_vectors[0])\n",
    "#         confusion_matrix(valid_label_vectors.ravel(), predicted_label_vectors.ravel())\n",
    "#         #print(confusion_matrix)\n",
    "#         score = f1_score(valid_label_vectors.ravel(), predicted_label_vectors.ravel(), average='weighted')\n",
    "#         print(score)\n",
    "#         if (score > best):\n",
    "#             model.save(\"best_model_\" + str(epoch) + \".h5\")\n",
    "#             best = score\n",
    "            \n",
    "\n",
    "        \n",
    "\n",
    "train_model(data, valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3249, 4)\n",
      "(3452, 4)\n"
     ]
    }
   ],
   "source": [
    "#Parsing and preprocessing of the training_data\n",
    "valid_data = process_data_file(\"NER-datasets/CONLL2003/valid.txt\")\n",
    "test_data = process_data_file(\"NER-datasets/CONLL2003/test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exlude_padded_class(scalar_label_vectors):\n",
    "    new_scalar_label_vectors = []\n",
    "    for instance in scalar_label_vectors:\n",
    "        temp = []\n",
    "        for label in instance:\n",
    "            if label != 9:\n",
    "                temp.append(label)\n",
    "        new_scalar_label_vectors.append(temp)\n",
    "    new_scalar_label_vectors = np.asarray(new_scalar_label_vectors)\n",
    "    return new_scalar_label_vectors\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  953    31    14    25     0    31    11     3     0     0]\n",
      " [   27 26904    24    10     3    10    16    12     1     0]\n",
      " [   21    40   550     9     0    21     2     2     1     0]\n",
      " [    2     7     3  1335     9     9     2     0     0     0]\n",
      " [    0    15     0     7   874     0     0     0     0     0]\n",
      " [   26     9     6    14     1  1337     1     1     2     0]\n",
      " [    5    32     0     1    13     3   450     4    14     0]\n",
      " [    0    36    11     1     2     1    11   171     4     0]\n",
      " [    0     4     0     0     3     4    10     3   164     0]\n",
      " [    0     0     0     0     0     0     0     0     0 15407]]\n"
     ]
    }
   ],
   "source": [
    " from sklearn.metrics import classification_report\n",
    "def metric(model_file, test_data):\n",
    "    model = load_model(model_file)\n",
    "    test_label_vectors = get_label_vectors(test_data)\n",
    "    test_word_feats, test_case_feats, test_pos_feats =  get_feature_vectors(test_data)\n",
    "    predicted_label_vectors = model.predict(([test_word_feats, test_case_feats, test_pos_feats]))\n",
    "    test_label_vectors =  convert_to_scalar(test_label_vectors)\n",
    "    #test_label_vectors = exlude_padded_class(test_label_vectors)\n",
    "    \n",
    "    predicted_label_vectors = convert_to_scalar(predicted_label_vectors)\n",
    "    \n",
    "    #predicted_label_vectors = exlude_padded_class(predicted_label_vectors)\n",
    "    labels = ['B-ORG', 'O', 'B-MISC', 'B-PER', 'I-PER', 'B-LOC', 'I-ORG', 'I-MISC', 'I-LOC', 'PADDED_LABEL']\n",
    "    \n",
    "    cf = confusion_matrix(test_label_vectors.ravel(), predicted_label_vectors.ravel())\n",
    "    print(cf)\n",
    "#     macro_score = f1_score(test_label_vectors.ravel(), predicted_label_vectors.ravel(), average='macro')\n",
    "#     micro_score = f1_score(test_label_vectors.ravel(), predicted_label_vectors.ravel(), average='micro')\n",
    "#     weighted_score = f1_score(test_label_vectors.ravel(), predicted_label_vectors.ravel(), average='weighted')\n",
    "#     print(macro_score, micro_score, weighted_score)\n",
    "    cf_report = classification_report(test_label_vectors.ravel(), predicted_label_vectors.ravel(), target_names= ['B-ORG', 'O', 'B-MISC', 'B-PER', 'I-PER', 'B-LOC', 'I-ORG', 'I-MISC', 'I-LOC', 'PADDED_LABEL'])\n",
    "    return cf_report\n",
    "\n",
    "model_file = \"best_model2.h5\"\n",
    "cf_report = metric(model_file,valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-ORG       0.92      0.89      0.91      1068\n",
      "           O       0.99      1.00      0.99     27007\n",
      "      B-MISC       0.90      0.85      0.88       646\n",
      "       B-PER       0.95      0.98      0.96      1367\n",
      "       I-PER       0.97      0.98      0.97       896\n",
      "       B-LOC       0.94      0.96      0.95      1397\n",
      "       I-ORG       0.89      0.86      0.88       522\n",
      "      I-MISC       0.87      0.72      0.79       237\n",
      "       I-LOC       0.88      0.87      0.88       188\n",
      "PADDED_LABEL       1.00      1.00      1.00     15407\n",
      "\n",
      " avg / total       0.99      0.99      0.99     48735\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Removing the padded_token class the over_all precision, recall and f-score of the system is as follows:\n",
    "Validation Data\n",
    "Precision: 91.67\n",
    "Recall: 90.12\n",
    "F-score: 91.23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1196    48    38    19     0    54    16     1     0     0]\n",
      " [   64 25772    57    22     1    30    48    51     8     0]\n",
      " [   40    37   393    11     0    22     3     6     0     0]\n",
      " [   22    17     2  1188    11    15     3     0     0     0]\n",
      " [    0     4     0     3   820     0     5     0     1     0]\n",
      " [   59    13    17     7     0  1223     4     2     4     0]\n",
      " [   17    44     1     1     8     2   503    14    18     0]\n",
      " [    0    34     2     0     1     0    19    93     4     0]\n",
      " [    0     9     0     0     2     5    26     1   161     0]\n",
      " [    0     0     0     0     0     0     0     0     0 19458]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.87      0.86      1372\n",
      "          1       0.99      0.99      0.99     26053\n",
      "          2       0.77      0.77      0.77       512\n",
      "          3       0.95      0.94      0.95      1258\n",
      "          4       0.97      0.98      0.98       833\n",
      "          5       0.91      0.92      0.91      1329\n",
      "          6       0.80      0.83      0.81       608\n",
      "          7       0.55      0.61      0.58       153\n",
      "          8       0.82      0.79      0.80       204\n",
      "          9       1.00      1.00      1.00     19458\n",
      "\n",
      "avg / total       0.98      0.98      0.98     51780\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def metric(model_file, test_data):\n",
    "    model = load_model(model_file)\n",
    "    test_label_vectors = get_label_vectors(test_data)\n",
    "    test_word_feats, test_case_feats, test_pos_feats =  get_feature_vectors(test_data)\n",
    "    predicted_label_vectors = model.predict(([test_word_feats, test_case_feats, test_pos_feats]))\n",
    "    test_label_vectors =  convert_to_scalar(test_label_vectors)\n",
    "    predicted_label_vectors = convert_to_scalar(predicted_label_vectors)\n",
    "    cf = confusion_matrix(test_label_vectors.ravel(), predicted_label_vectors.ravel())\n",
    "    print(cf)\n",
    "#     macro_score = f1_score(test_label_vectors.ravel(), predicted_label_vectors.ravel(), average='macro')\n",
    "#     micro_score = f1_score(test_label_vectors.ravel(), predicted_label_vectors.ravel(), average='micro')\n",
    "#     weighted_score = f1_score(test_label_vectors.ravel(), predicted_label_vectors.ravel(), average='weighted')\n",
    "#     print(macro_score, micro_score, weighted_score)\n",
    "    print(classification_report(test_label_vectors.ravel(), predicted_label_vectors.ravel()))\n",
    "\n",
    "model_file = \"best_model2.h5\"\n",
    "metric(model_file,test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing the padded_token class the over_all precision, recall and f-score of the system is as follows:\n",
    "Validation Data\n",
    "Precision: 84.67\n",
    "Recall: 85.56\n",
    "F-score: 85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
