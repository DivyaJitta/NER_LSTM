{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14986, 4)\n"
     ]
    }
   ],
   "source": [
    "#Parsing and preprocessing of the data\n",
    "import numpy as np\n",
    "f = open(\"NER-datasets/CONLL2003/train.txt\", \"r\")\n",
    "tokens = []\n",
    "pos = []\n",
    "chunks = []\n",
    "ner_lables = []\n",
    "data = []\n",
    "for line in  f:\n",
    "    if line == \"\\n\":\n",
    "        \n",
    "        sent = []\n",
    "        sent.append(tokens)\n",
    "        sent.append(pos)\n",
    "        sent.append(chunks)\n",
    "        sent.append(ner_lables)\n",
    "        data.append(sent)\n",
    "        \n",
    "        # refresh the arrays\n",
    "        tokens = []\n",
    "        pos = []\n",
    "        chunks = []\n",
    "        ner_lables = []\n",
    "        \n",
    "    else:\n",
    "        try:\n",
    "            parts = line.strip(\"\\n\").split(\" \")\n",
    "            tokens.append(parts[0].lower())\n",
    "            pos.append(parts[1])\n",
    "            chunks.append(parts[2])\n",
    "            ner_lables.append(parts[3])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        \n",
    "data = np.asarray(data)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[list(['-docstart-']) list(['-X-']) list(['-X-']) list(['O'])]\n"
     ]
    }
   ],
   "source": [
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[list(['eu', 'rejects', 'german', 'call', 'to', 'boycott', 'british', 'lamb', '.'])\n",
      " list(['NNP', 'VBZ', 'JJ', 'NN', 'TO', 'VB', 'JJ', 'NN', '.'])\n",
      " list(['B-NP', 'B-VP', 'B-NP', 'I-NP', 'B-VP', 'I-VP', 'B-NP', 'I-NP', 'O'])\n",
      " list(['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O'])]\n"
     ]
    }
   ],
   "source": [
    "print(data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Intializing some global vars\n",
    "Embeddings = {}\n",
    "word_emb_dim  = 50\n",
    "embedding_file = \"embeddings/glove.6B.50d.txt\"\n",
    "def LoadEmbeddings(embedding_file):\n",
    "    global Embeddings, word_emb_dim\n",
    "    f = open(embedding_file, \"r\", encoding= \"utf-8\", errors= \"ignore\")\n",
    "    for line in f :\n",
    "        tokens = line.strip(\"\\n\").split()\n",
    "        word = tokens[0]\n",
    "        vec = tokens[1:]\n",
    "        vec = \" \". join(vec)\n",
    "        Embeddings[word] = vec\n",
    "    Embeddings[\"zero_vec\"] = \"0.0\" * word_emb_dim\n",
    "    f.close()\n",
    "    \n",
    "    g = open (\"embeddings/EmbedDict.pkl\", \"wb\")\n",
    "    pickle.dump(Embeddings, g)\n",
    "    g.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "LoadEmbeddings(\"embeddings/glove.6B.50d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "GloveEmbeddings = {}\n",
    "max_words = 15\n",
    "def get_feature_vector(words):\n",
    "    global max_words, GloveEmbeddings\n",
    "    word_count = len(words)\n",
    "    left_over = max_words - word_count\n",
    "    if (left_over > 0): #padding req\n",
    "        words += [\"zero_vec\"] * left_over\n",
    "    #remove extra words\n",
    "    words = words[:max_words]\n",
    "    \n",
    "    # now obtain the feature vector\n",
    "    feat_vec = []\n",
    "    for word in words:\n",
    "        try:\n",
    "            glove_vec = [float(v for v in GloveEmbeddings[word].split())]\n",
    "        except:\n",
    "            glove_vec = [float(v for v in GloveEmbeddings[\"zero_vec\"].split())]\n",
    "        feat_vec.append(glove_vec)\n",
    "    return feat_vec\n",
    "\n",
    "def get_word_embeddings(data):\n",
    "    #load the embeddings from the pickle file\n",
    "    global GloveEmbeddings\n",
    "    f = open(\"EmbedDict.pkl\", \"rb\")\n",
    "    GloveEmbeddings = pickle.load(f)\n",
    "    f.close()\n",
    "    \n",
    "    for instance in data:\n",
    "        words = instance[0]\n",
    "        feature_vector = get_feature_vector(words)\n",
    "        \n",
    "def  get_case_embeddings(word):\n",
    "    case_emb_dim = 7\n",
    "    case_map = {'numeric': 0, 'all_lower': 1, 'all_upper': 2, 'initial_upper': 3, 'other': 4, 'mainly_numeric': 5,\n",
    "                    'contains_digit': 6}#, 'PADDING_TOKEN': 7}\n",
    "    num_of_digits = 0\n",
    "    for char in word:\n",
    "        if char.isdigit():\n",
    "            num_of_digits += 1\n",
    "    number_part = float(num_of_digits/len(word))\n",
    "    \n",
    "    casing = 'other'\n",
    "    \n",
    "    if num_of_digits == len(word):\n",
    "        casing = \"numeric\"\n",
    "    elif number_part >= 0.5:\n",
    "        casing = \"mainly_numeric\"\n",
    "    elif num_of_digits > 0:\n",
    "        casing = \"cantains_digit\"\n",
    "    elif word.isupper():\n",
    "        casing = \"all_upper\"\n",
    "    elif word.islower():\n",
    "        casing = \"all_lower\"\n",
    "    elif word[0].lower():\n",
    "        casing = \"initial_upper\"\n",
    "    \n",
    "    case_vector = [0]*7\n",
    "    case_embedding = case_vector[case_map[casing]] = 1\n",
    "    \n",
    "    return case_embedding\n",
    "\n",
    "def create_pos_embeddings(data):\n",
    "    pos_statistics = {}\n",
    "    pos_set = []\n",
    "    for instance in data:\n",
    "        pos_tags = instance[2]\n",
    "        for tag in pos_tags:\n",
    "            try:\n",
    "                pos_statistics[tag] = pos_statistics + 1\n",
    "            except:\n",
    "                pos_statistics[tag] = 1\n",
    "    for pos in pos_statistics:\n",
    "        pos_set.append(pos)\n",
    "    \n",
    "    print(pos_set)\n",
    "    print(len(pos_set))\n",
    "    pos_map = {}\n",
    "    for pos_idx in range(len(pos_set)):\n",
    "        pos_vec = [0]* len(pos_set)\n",
    "        pos_vec[pos_idx] = 1\n",
    "        pos_map[pos_set[pos_idx]] = pos_vec\n",
    "    \n",
    "    f = open(\"POSEmbeddings.pickle\", \"wb\")\n",
    "    pickle.dump(pos_map, f)\n",
    "    f.close()\n",
    "    \n",
    "def create_label_embeddings(data):\n",
    "    label_set = []\n",
    "    label_statistics = {}\n",
    "    for instance in data:\n",
    "        labels = instance[3]\n",
    "        for label in labels:\n",
    "            try:\n",
    "                label_statistics[label] = label_statistics[label] + 1\n",
    "            except:\n",
    "                label_statistics[label] = 1\n",
    "    print(label_statistics)\n",
    "    for label in label_statistics:\n",
    "        label_set.append(label)\n",
    "    print(label_set)\n",
    "    print(len(label_set))\n",
    "    \n",
    "    label_map = {}\n",
    "    \n",
    "    for label_idx in range(len(label_set)):\n",
    "        label_vec = [0] * len(label_set)\n",
    "        label_vec[label_idx] = 1\n",
    "        label_map[label_set[label_idx]] = label_vec\n",
    "    \n",
    "    f = open(\"LabelEmbed.pickle\", \"wb\")\n",
    "    pickle.dump(label_map, f)\n",
    "    f.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-X-', 'B-NP', 'B-VP', 'I-NP', 'I-VP', 'O', 'B-PP', 'B-SBAR', 'B-ADJP', 'I-ADJP', 'B-ADVP', 'B-PRT', 'B-CONJP', 'I-CONJP', 'I-PP', 'B-INTJ', 'I-ADVP', 'B-LST', 'I-SBAR', 'I-LST', 'I-INTJ']\n",
      "21\n"
     ]
    }
   ],
   "source": [
    "#create_label_embeddings(data) # Label Statistics\n",
    "create_pos_embeddings(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model, load_model, save_model\n",
    "from keras.layers import Bidirectional, LSTM, Dense, Activation, Input, concatenate, TimeDistributed\n",
    "def Build_Model(sent_max_words = 15, word_emb_dim = 50 , case_emb_dim = 7, pos_emb_dim = 21):\n",
    "    lstm_dim = 200\n",
    "    num_classes = 5\n",
    "    #word input\n",
    "    word_input = Input(shape=(sent_max_words, word_emb_dim))\n",
    "    #case input\n",
    "    case_input = Input(shape=(sent_max_words, case_emb_dim))\n",
    "    #pos_input\n",
    "    pos_input = Input(shape=(sent_max_words, pos_emb_dim))\n",
    "    #Concatenate the three inputs\n",
    "    merged_input = concatenate([word_input, case_input, pos_input])\n",
    "    #pass the merged input to a BiLSTM\n",
    "    lstm_output = Bidirectional(LSTM(lstm_dim, return_sequences=True, dropout = 0.2),merge_mode=None)(merged_input)\n",
    "    merged_output = concatenate([lstm_output[0], lstm_output[1]], axis = 2)\n",
    "    #Add a Time Distributed Layer\n",
    "    output = TimeDistributed(Dense(num_classes, activation = \"softmax\"), name = \"Softmax_Layer\")(merged_output)\n",
    "    \n",
    "    ## Model ##\n",
    "    model = Model(inputs = [word_input, case_input, pos_input], outputs = output)\n",
    "    model.summary()\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_27 (InputLayer)           (None, 15, 50)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_28 (InputLayer)           (None, 15, 7)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_29 (InputLayer)           (None, 15, 21)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_14 (Concatenate)    (None, 15, 78)       0           input_27[0][0]                   \n",
      "                                                                 input_28[0][0]                   \n",
      "                                                                 input_29[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_7 (Bidirectional) [(None, 15, 200), (N 446400      concatenate_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)    (None, 15, 400)      0           bidirectional_7[0][0]            \n",
      "                                                                 bidirectional_7[0][1]            \n",
      "__________________________________________________________________________________________________\n",
      "Softmax_Layer (TimeDistributed) (None, 15, 5)        2005        concatenate_15[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 448,405\n",
      "Trainable params: 448,405\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Build_Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
